
@article{Agnoli2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  date = {2017},
  journaltitle = {PLOS ONE},
  volume = {12},
  pages = {e0172792},
  publisher = {Public Library of Science},
  doi = {10.1371/journal.pone.0172792},
  url = {https://doi.org/10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  bdsk-url-2 = {http://dx.doi.org/10.1371/journal.pone.0172792},
  number = {3}
}

@article{Allen2019,
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  author = {Allen, Christopher and Mehler, David M. A.},
  date = {2019-05-01},
  journaltitle = {PLOS Biology},
  volume = {17},
  pages = {e3000246},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.3000246},
  url = {http://dx.plos.org/10.1371/journal.pbio.3000246},
  urldate = {2019-08-28},
  langid = {english},
  number = {5}
}

@book{AssociationforPsychologicalScience,
  title = {Registered {{Replication Reports}}},
  author = {{Association for Psychological Science}},
  url = {https://www.psychologicalscience.org/publications/replication},
  urldate = {2019-01-27}
}

@article{Atkinson1982,
  title = {Statistical Significance, Reviewer Evaluations, and the Scientific Process: {{Is}} There a (Statistically) Significant Relationship?},
  shorttitle = {Statistical Significance, Reviewer Evaluations, and the Scientific Process},
  author = {Atkinson, Donald R. and Furlong, Michael J. and Wampold, Bruce E.},
  date = {1982},
  journaltitle = {Journal of Counseling Psychology},
  volume = {29},
  pages = {189--194},
  issn = {0022-0167},
  doi = {10.1037/0022-0167.29.2.189},
  url = {http://content.apa.org/journals/cou/29/2/189},
  urldate = {2020-01-05},
  langid = {english},
  number = {2}
}

@article{Atkinson1982a,
  title = {Statistical Significance, Reviewer Evaluations, and the Scientific Process: {{Is}} There a (Statistically) Significant Relationship?},
  shorttitle = {Statistical Significance, Reviewer Evaluations, and the Scientific Process},
  author = {Atkinson, Donald R. and Furlong, Michael J. and Wampold, Bruce E.},
  date = {1982},
  journaltitle = {Journal of Counseling Psychology},
  volume = {29},
  pages = {189--194},
  issn = {0022-0167},
  doi = {10.1037/0022-0167.29.2.189},
  url = {http://content.apa.org/journals/cou/29/2/189},
  urldate = {2020-01-05},
  langid = {english},
  number = {2}
}

@book{CenterforOpenSciencea,
  title = {{{OSF}} - {{Registered Reports}}},
  author = {{Center for Open Science}},
  url = {https://www.zotero.org/groups/479248/osf/items/collectionKey/KEJP68G9?},
  urldate = {2018-10-26}
}

@article{Chambers2013,
  title = {Registered Reports: A New Publishing Initiative at {{Cortex}}},
  author = {Chambers, C.D.},
  date = {2013},
  volume = {49},
  pages = {609--610},
  url = {http://orca.cf.ac.uk/45177/1/Chambers_Cortex_2013b_GreenOA.pdf},
  number = {3}
}

@article{Chambers2015,
  title = {Registered {{Reports}}: {{Realigning}} Incentives in Scientific Publishing},
  author = {Chambers, C.D. and Dienes, Z. and McIntosh, R.D. and Rotshtein, P. and Willmes, K.},
  date = {2015},
  journaltitle = {Cortex},
  volume = {66},
  pages = {1--2},
  issn = {19738102},
  doi = {10.1016/j.cortex.2015.03.022},
  abstract = {This editorial present views on realigning incentives in scientific publishing. As editors recognize this important moment for Cortex, they also take the opportunity to reiterate our view that Registered Reports should not be seen as a one-shot cure for reproducibility problems in science. The applicability of Registered Reports to different sub-fields within neuropsychology and cognitive neuroscience remains to be established; for instance, studies that rely exclusively on exploration rather than deductive hypothesis testing may not be compatible. Registered Reports present no threat to exploratory science in cases where studies include a mixture of both hypothesis testing and exploratory analyses, authors are welcome to report the outcomes of the unregistered analyses, as Sassenhagen and Bornkessel-Schlesewsky do in the current issue. Pre-registration simply allows readers to distinguish the outcomes based on a priori hypothesis testing from post hoc exploration. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  eprint = {25892410},
  eprinttype = {pmid}
}

@article{Cristea2018,
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  date = {2018-05-15},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {13},
  pages = {e0197440},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0197440},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197440},
  urldate = {2019-10-01},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  keywords = {Analysis of variance,Bayesian method,Bayesian statistics,Computer software,Meta-analysis,Scientific publishing,Software tools,Statistical data},
  langid = {english},
  number = {5}
}

@article{Fanelli2010,
  title = {"{{Positive}}" Results Increase down the Hierarchy of the Sciences},
  author = {Fanelli, Daniele},
  editor = {Scalas, Enrico},
  date = {2010-04},
  journaltitle = {PLoS ONE},
  volume = {5},
  pages = {e10068},
  issn = {19326203},
  doi = {10.1371/journal.pone.0010068},
  url = {http://dx.plos.org/10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the "hardness" of scientific research–i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors–is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a "positive" (full or partial) or "negative" support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in "softer" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  eprint = {20383332},
  eprinttype = {pmid},
  number = {4}
}

@article{Fanelli2012,
  title = {Negative Results Are Disappearing from Most Disciplines and Countries},
  author = {Fanelli, Daniele},
  date = {2012-03},
  journaltitle = {Scientometrics},
  volume = {90},
  pages = {891--904},
  issn = {01389130},
  doi = {10.1007/s11192-011-0494-7},
  url = {http://link.springer.com/10.1007/s11192-011-0494-7},
  abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ‘‘tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
  keywords = {Bias,Competition,Misconduct,Publication,Publish or perish,Research evaluation},
  number = {3}
}

@article{Ferguson2012,
  title = {A {{Vast Graveyard}} of {{Undead Theories}}: {{Publication Bias}} and {{Psychological Science}}'s {{Aversion}} to the {{Null}}},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  date = {2012},
  journaltitle = {Perspectives on Psychological Science},
  volume = {7},
  pages = {555--561},
  issn = {17456916},
  doi = {10.1177/1745691612459059},
  abstract = {Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science's capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.},
  eprint = {26168112},
  eprinttype = {pmid},
  keywords = {fail-safe number,falsification,meta-analyses,null hypothesis significance testing,publication bias},
  number = {6}
}

@article{Fiedler2016,
  title = {Questionable {{Research Practices Revisited}}},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  date = {2016-01},
  journaltitle = {Social Psychological and Personality Science},
  volume = {7},
  pages = {45--52},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615612150},
  url = {http://journals.sagepub.com/doi/10.1177/1948550615612150},
  urldate = {2019-09-23},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  langid = {english},
  number = {1}
}

@article{Franco2014,
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  shorttitle = {Publication Bias in the Social Sciences},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  date = {2014},
  journaltitle = {Science},
  volume = {345},
  pages = {1502--1505},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1255484},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1255484},
  urldate = {2018-06-19},
  keywords = {publication bias,replication crisis},
  langid = {english},
  number = {6203}
}

@article{Franco2016,
  title = {Underreporting in {{Psychology Experiments}}: {{Evidence From}} a {{Study Registry}}},
  shorttitle = {Underreporting in {{Psychology Experiments}}},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  date = {2016},
  journaltitle = {Social Psychological and Personality Science},
  volume = {7},
  pages = {8--12},
  issn = {1948-5506, 1948-5514},
  doi = {10.1177/1948550615598377},
  url = {http://journals.sagepub.com/doi/10.1177/1948550615598377},
  urldate = {2018-06-19},
  keywords = {publication bias,replication crisis},
  langid = {english},
  number = {1}
}

@article{Greenwald1975,
  title = {Consequences of {{Prejudice Against}} the {{Null Hypothesis}}},
  author = {Greenwald, Anthony G.},
  date = {1975},
  journaltitle = {Psychological Bulletin},
  volume = {82},
  pages = {1--20},
  number = {1}
}

@article{Hardwicke2018,
  title = {Mapping the Universe of Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  date = {2018-10},
  journaltitle = {Nature Human Behaviour},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0444-y},
  url = {http://www.nature.com/articles/s41562-018-0444-y https://osf.io/preprints/bitss/fzpcy/},
  keywords = {journal policy,meta-research,open science,pre-registration,Registered Reports}
}

@article{John2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  date = {2012-05},
  journaltitle = {Psychological Science},
  volume = {23},
  pages = {524--532},
  issn = {14679280},
  doi = {10.1177/0956797611430953},
  url = {http://journals.sagepub.com/doi/10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  eprint = {22508865},
  eprinttype = {pmid},
  keywords = {disclosure,judgment,methodology,professional standards},
  number = {5}
}

@article{Jonas2016,
  title = {How Can Preregistration Contribute to Research in Our Field?},
  author = {Jonas, Kai J. and Cesario, Joseph},
  date = {2016},
  journaltitle = {Comprehensive Results in Social Psychology},
  volume = {1},
  pages = {1--7},
  issn = {2374-3603, 2374-3611},
  doi = {10.1080/23743603.2015.1070611},
  url = {https://www.tandfonline.com/doi/full/10.1080/23743603.2015.1070611},
  urldate = {2018-05-23},
  langid = {english},
  number = {1-3}
}

@article{Kerr1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  author = {Kerr, Norbert L.},
  date = {1998-08},
  journaltitle = {Personality and Social Psychology Review},
  volume = {2},
  pages = {196--217},
  issn = {10888683},
  doi = {10.1207/s15327957pspr0203_4},
  url = {http://journals.sagepub.com/doi/10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  eprint = {15647155},
  eprinttype = {pmid},
  number = {3}
}

@article{Kohler2019,
  title = {Play {{It Again}}, {{Sam}}! {{An Analysis}} of {{Constructive Replication}} in the {{Organizational Sciences}}},
  author = {Köhler, Tine and Cortina, Jose M.},
  date = {2019-04-24},
  journaltitle = {Journal of Management},
  pages = {014920631984398},
  issn = {0149-2063, 1557-1211},
  doi = {10.1177/0149206319843985},
  url = {http://journals.sagepub.com/doi/10.1177/0149206319843985},
  urldate = {2019-12-19},
  langid = {english}
}

@article{Lakens2018a,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
  date = {2018},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {259--269},
  issn = {10959513},
  doi = {10.1016/j.ympev.2015.01.015},
  url = {http://journals.sagepub.com/doi/10.1177/2515245918770963},
  abstract = {The North American carnivorous pitcher plant genus Sarracenia (Sarraceniaceae) is a relatively young clade (\textbackslash{}textless3 million years ago) displaying a wide range of morphological diversity in complex trapping structures. This recently radiated group is a promising system to examine the structural evolution and diversification of carnivorous plants; however, little is known regarding evolutionary relationships within the genus. Previous attempts at resolving the phylogeny have been unsuccessful, most likely due to few parsimony-informative sites compounded by incomplete lineage sorting. Here, we applied a target enrichment approach using multiple accessions to assess the relationships of Sarracenia species. This resulted in 199 nuclear genes from 75 accessions covering the putative 8-11 species and 8 subspecies/varieties. In addition, we recovered 42. kb of plastome sequence from each accession to estimate a cpDNA-derived phylogeny. Unsurprisingly, the cpDNA had few parsimony-informative sites (0.5\%) and provided little information on species relationships. In contrast, use of the targeted nuclear loci in concatenation and coalescent frameworks elucidated many relationships within Sarracenia even with high heterogeneity among gene trees. Results were largely consistent for both concatenation and coalescent approaches. The only major disagreement was with the placement of the purpurea complex. Moreover, results suggest an Appalachian massif biogeographic origin of the genus. Overall, this study highlights the utility of target enrichment using multiple accessions to resolve relationships in recently radiated taxa.},
  eprint = {25689607},
  eprinttype = {pmid},
  keywords = {equivalence testing,falsification,frequentist,null hypothesis,null-hypothesis significance test,open,power,tost},
  number = {2}
}

@article{Lakens2019b,
  title = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}: {{A Conceptual Analysis}}},
  shorttitle = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}},
  author = {Lakens, Daniel},
  date = {2019-11-18},
  doi = {10.31234/osf.io/jbh4w},
  url = {https://psyarxiv.com/jbh4w/},
  urldate = {2019-12-30},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
  keywords = {meta-science,preregistration}
}

@article{Mahoney1977,
  title = {Publication {{Prejudices}}: {{An Experimental Study}} of {{Confirmatory Bias}} in the {{Peer Review System}}},
  author = {Mahoney, Michael J.},
  date = {1977},
  journaltitle = {Cognitive Therapy and Research},
  volume = {1},
  pages = {161--175},
  doi = {10.1007/BF01173636},
  url = {https://dx.doi.org/10.1007/BF01173636},
  number = {2}
}

@article{Makel2012,
  title = {Replications in {{Psychology Research}}: {{How Often Do They Really Occur}}?},
  shorttitle = {Replications in {{Psychology Research}}},
  author = {Makel, Matthew C. and Plucker, Jonathan A. and Hegarty, Boyd},
  date = {2012-11-07},
  journaltitle = {Perspectives on Psychological Science},
  doi = {10.1177/1745691612460688},
  url = {https://journals.sagepub.com/doi/full/10.1177/1745691612460688},
  urldate = {2020-01-03},
  abstract = {Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention ...},
  keywords = {meta-science,replication},
  langid = {english}
}

@article{Maxwell2004,
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}.},
  shorttitle = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}},
  author = {Maxwell, Scott E.},
  date = {2004},
  journaltitle = {Psychological Methods},
  volume = {9},
  pages = {147--163},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/1082-989X.9.2.147},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.9.2.147},
  urldate = {2019-01-31},
  keywords = {meta-science,power,replication crisis,stats},
  langid = {english},
  number = {2}
}

@article{Motyl2017,
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  shorttitle = {The State of Social and Personality Science},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  date = {2017},
  journaltitle = {Journal of Personality and Social Psychology},
  volume = {113},
  pages = {34--58},
  issn = {1939-1315, 0022-3514},
  doi = {10.1037/pspa0000084},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/pspa0000084},
  urldate = {2018-06-14},
  langid = {english},
  number = {1}
}

@article{Mueller-Langer2019,
  title = {Replication Studies in Economics—{{How}} Many and Which Papers Are Chosen for Replication, and Why?},
  author = {Mueller-Langer, Frank and Fecher, Benedikt and Harhoff, Dietmar and Wagner, Gert G.},
  date = {2019-02-01},
  journaltitle = {Research Policy},
  shortjournal = {Research Policy},
  volume = {48},
  pages = {62--83},
  issn = {0048-7333},
  doi = {10.1016/j.respol.2018.07.019},
  url = {http://www.sciencedirect.com/science/article/pii/S0048733318301847},
  urldate = {2019-12-19},
  abstract = {We investigate how often replication studies are published in empirical economics and what types of journal articles are replicated. We find that between 1974 and 2014 0.1\% of publications in the top 50 economics journals were replication studies. We consider the results of published formal replication studies (whether they are negating or reinforcing) and their extent: Narrow replication studies are typically devoted to mere replication of prior work, while scientific replication studies provide a broader analysis. We find evidence that higher-impact articles and articles by authors from leading institutions are more likely to be replicated, whereas the replication probability is lower for articles that appeared in top 5 economics journals. Our analysis also suggests that mandatory data disclosure policies may have a positive effect on the incidence of replication.},
  keywords = {Economic methodology,Economics of science,Replication,Science policy},
  langid = {english},
  number = {1}
}

@article{Nosek2014,
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  shorttitle = {Registered {{Reports}}},
  author = {Nosek, Brian A. and Lakens, Daniël},
  date = {2014},
  journaltitle = {Social Psychology},
  volume = {45},
  pages = {137--141},
  issn = {1864-9335, 2151-2590},
  doi = {10.1027/1864-9335/a000192},
  url = {http://econtent.hogrefe.com/doi/abs/10.1027/1864-9335/a000192},
  urldate = {2018-05-23},
  langid = {english},
  number = {3}
}

@article{OSC2015,
  title = {Estimating the Reproducibility of Psychological Science},
  author = {{Open Science Collaboration}},
  date = {2015-08-28},
  journaltitle = {Science},
  volume = {349},
  pages = {aac4716},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac4716},
  url = {https://science.sciencemag.org/content/349/6251/aac4716},
  urldate = {2019-12-19},
  abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  eprint = {26315443},
  eprinttype = {pmid},
  langid = {english},
  number = {6251}
}

@article{Pridemore2018,
  title = {Replication in {{Criminology}} and the {{Social Sciences}}},
  author = {Pridemore, William Alex and Makel, Matthew C. and Plucker, Jonathan A.},
  date = {2018-01-13},
  journaltitle = {Annual Review of Criminology},
  shortjournal = {Annu. Rev. Criminol.},
  volume = {1},
  pages = {19--38},
  issn = {2572-4568},
  doi = {10.1146/annurev-criminol-032317-091849},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-criminol-032317-091849},
  urldate = {2019-12-19},
  abstract = {Replication is a hallmark of science. In recent years, some medical sciences and behavioral sciences struggled with what came to be known as replication crises. As a field, criminology has yet to address formally the threats to our evidence base that might be posed by large-scale and systematic replication attempts, although it is likely we would face challenges similar to those experienced by other disciplines. In this review, we outline the basics of replication, summarize reproducibility problems found in other fields, undertake an original analysis of the amount and nature of replication studies appearing in criminology journals, and consider how criminology can begin to assess more formally the robustness of our knowledge through encouraging a culture of replication.},
  keywords = {criminology,meta-science,replication},
  number = {1}
}

@book{Rohatgi2018,
  title = {{{WebPlotDigitizer}} - {{Web Based Plot Digitizer}}},
  author = {Rohatgi, A.},
  date = {2018},
  location = {{Austin, Texas, USA}},
  url = {https://automeris.io/WebPlotDigitizer}
}

@article{Rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  author = {Rosenthal, Robert},
  date = {1979},
  journaltitle = {Psychological Bulletin},
  volume = {86},
  pages = {638--641},
  issn = {00332909},
  doi = {10.1037/0033-2909.86.3.638},
  url = {http://content.apa.org/journals/bul/86/3/638},
  abstract = {For any gien research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type 1 errors, while the file drawers are filled with the 95\% of the studies that show non-significant resluts. Quantitative procedures for computing the tolerance for filed and future results are reported and illustrated, and the implications are discussed.},
  eprint = {53},
  eprinttype = {pmid},
  keywords = {tolerance for null results bias in publication of},
  number = {3}
}

@online{RRRwebsite,
  title = {Registered {{Replication Reports}}},
  journaltitle = {Association for Psychological Science - APS},
  url = {https://www.psychologicalscience.org/publications/replication},
  urldate = {2019-12-28},
  abstract = {Quick Links

 	Mission Statement
 	Article Type Description
 	Instructions for Authors
 	Instructions for Reviewers
 	Ongoing Replication Projects

Mission Statement
Replicability is a cornerstone of science. Yet replication studies rarely appear in psychology journals. The new Registered Replication Reports …},
  langid = {american}
}

@book{RStudioTeam2019,
  title = {{{RStudio}}: {{Integrated}} Development Environment for r},
  author = {{RStudio Team}},
  date = {2019},
  location = {{Boston, MA}},
  url = {http://www.rstudio.com/},
  organization = {{RStudio, Inc.}}
}

@article{Simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  date = {2011-11},
  journaltitle = {Psychological Science},
  volume = {22},
  pages = {1359--1366},
  issn = {14679280},
  doi = {10.1177/0956797611417632},
  url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  eprint = {22006061},
  eprinttype = {pmid},
  keywords = {disclosure,methodology,motivated reasoning,publication},
  number = {11}
}

@article{Simons2014,
  title = {An {{Introduction}} to {{Registered Replication Reports}} at {{{\emph{Perspectives}}}}{\emph{ on }}{{{\emph{Psychological Science}}}}},
  author = {Simons, Daniel J. and Holcombe, Alex O. and Spellman, Barbara A.},
  date = {2014},
  journaltitle = {Perspectives on Psychological Science},
  volume = {9},
  pages = {552--555},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691614543974},
  url = {http://journals.sagepub.com/doi/10.1177/1745691614543974},
  urldate = {2018-06-08},
  langid = {english},
  number = {5}
}

@article{Simons2018,
  title = {Introducing {{Advances}} in {{Methods}} and {{Practices}} in {{Psychological Science}}},
  author = {Simons, Daniel J.},
  date = {2018-03-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  pages = {3--6},
  issn = {2515-2459},
  doi = {10.1177/2515245918757424},
  url = {https://doi.org/10.1177/2515245918757424},
  urldate = {2019-09-27},
  langid = {english},
  number = {1}
}

@article{Sterling1959,
  title = {Publication {{Decisions}} and Their {{Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance}}—or {{Vice Versa}}},
  author = {Sterling, Theodore D.},
  date = {1959-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {54},
  pages = {30--34},
  issn = {1537274X},
  doi = {10.1080/01621459.1959.10501497},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1959.10501497},
  abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an "error of the first kind"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.},
  eprint = {25246403},
  eprinttype = {pmid},
  number = {285}
}

@article{Sterling1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  shorttitle = {Publication {{Decisions Revisited}}},
  author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
  date = {1995-02},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  volume = {49},
  pages = {108},
  issn = {00031305},
  doi = {10.2307/2684823},
  url = {https://www.jstor.org/stable/2684823?origin=crossref},
  urldate = {2019-08-15},
  keywords = {meta-science,NHST,publication bias},
  number = {1}
}

@article{Szucs2017,
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  editor = {Wagenmakers, Eric-Jan},
  date = {2017},
  journaltitle = {PLOS Biology},
  volume = {15},
  pages = {e2000797},
  issn = {1545-7885},
  doi = {10.1371/journal.pbio.2000797},
  url = {http://dx.plos.org/10.1371/journal.pbio.2000797},
  urldate = {2018-06-14},
  langid = {english},
  number = {3}
}

@article{vanAssen2014,
  title = {Why {{Publishing Everything Is More Effective}} than {{Selective Publishing}} of {{Statistically Significant Results}}},
  author = {van Assen, Marcel A. L. M. and van Aert, Robbie C. M. and Nuijten, Michèle B. and Wicherts, Jelte M.},
  date = {2014-01-17},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {9},
  pages = {e84896},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0084896},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084896},
  urldate = {2019-12-19},
  abstract = {Background De Winter and Happee [1] examined whether science based on selective publishing of significant results may be effective in accurate estimation of population effects, and whether this is even more effective than a science in which all results are published (i.e., a science without publication bias). Based on their simulation study they concluded that “selective publishing yields a more accurate meta-analytic estimation of the true effect than publishing everything, (and that) publishing nonreplicable results while placing null results in the file drawer can be beneficial for the scientific collective” (p.4). Methods and Findings Using their scenario with a small to medium population effect size, we show that publishing everything is more effective for the scientific collective than selective publishing of significant results. Additionally, we examined a scenario with a null effect, which provides a more dramatic illustration of the superiority of publishing everything over selective publishing. Conclusion Publishing everything is more effective than only reporting significant outcomes.},
  keywords = {Meta-analysis,meta-science,Normal distribution,publication bias,Publication ethics,Scientific publishing,Scientists,Simulation and modeling,Social sciences,Statistical methods},
  langid = {english},
  number = {1},
  options = {useprefix=true}
}

@article{Veldkamp2018,
  title = {Preprint "{{Ensuring}} the Quality and Specificity of Preregistrations"},
  author = {Veldkamp, Coosje Lisabet Sterre and Bakker, Marjan and van Assen, Marcel A.L.M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Soderberg, Courtney K. and Mellor, David and Nosek, Brian A. and Wicherts, Jelte},
  date = {2018},
  pages = {1--30},
  doi = {10.31234/OSF.IO/CDGYH},
  url = {https://psyarxiv.com/cdgyh},
  options = {useprefix=true}
}

@article{Wilkinson1999,
  title = {Statistical Methods in Psychology Journals: {{Guidelines}} and Explanations},
  author = {Wilkinson, Leland},
  date = {1999},
  journaltitle = {American Psychologist},
  volume = {54},
  pages = {594--604},
  issn = {0003066X},
  doi = {10.1037/0003-066X.54.8.594},
  abstract = {In the light of continuing debate over the applications of significance testing in psychology journals and following the publication of J. Cohen's (1994) article, the Board of Scientific Affairs (BSA) of the American Psychological Association (APA) convened a committee called the Task Force on Statistical Interference (TFSI) whose charge was "to elucidate some of the controversial issues surrounding applications of statistics including significance testing and its alternatives; alternative underlying models and data transformation; and newer methods made possible by powerful computers" (BSA, personal communication, February 28, 1996). After extensive discussion, the BSA recommended that publishing an article in American Psychologist, as a way to initiate discussion in the field about changes in current practices of data analysis and reporting may be appropriate. This report follows that request. Following each guideline are comments, explanations, or elaborations assembled by L. Wilkinson for the task force and under its review. The report is concerned with the use of statistical methods only and is not meant as an assessment of research methods in general. The title and format of the report are adapted from an article by J. C. Bailar and F. Mosteller (1988).},
  eprint = {18793039},
  eprinttype = {pmid},
  number = {8}
}

@article{Winter2013,
  title = {Why {{Selective Publication}} of {{Statistically Significant Results Can Be Effective}}},
  author = {de Winter, Joost and Happee, Riender},
  date = {2013-06-20},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  pages = {e66463},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0066463},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0066463},
  urldate = {2019-12-19},
  abstract = {Concerns exist within the medical and psychological sciences that many published research findings are not replicable. Guidelines accordingly recommend that the file drawer effect should be eliminated and that statistical significance should not be a criterion in the decision to submit and publish scientific results. By means of a simulation study, we show that selectively publishing effects that differ significantly from the cumulative meta-analytic effect evokes the Proteus phenomenon of poorly replicable and alternating findings. However, the simulation also shows that the selective publication approach yields a scientific record that is content rich as compared to publishing everything, in the sense that fewer publications are needed for obtaining an accurate meta-analytic estimation of the true effect. We conclude that, under the assumption of self-correcting science, the file drawer effect can be beneficial for the scientific collective.},
  keywords = {Heart,Medicine and health sciences,Mental health and psychiatry,Meta-analysis,meta-science,power,publication bias,Replication studies,Research design,Research validity,Scientific publishing,stats},
  langid = {english},
  number = {6}
}

@article{Wiseman2019,
  title = {Registered Reports: An Early Example and Analysis},
  shorttitle = {Registered Reports},
  author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
  date = {2019-01-16},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  volume = {7},
  pages = {e6232},
  issn = {2167-8359},
  doi = {10.7717/peerj.6232},
  url = {https://peerj.com/articles/6232},
  urldate = {2019-12-28},
  abstract = {The recent ‘replication crisis’ in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting ‘Registered Reports’, wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson’s pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
  keywords = {history of science,meta-science,parapsychology,RRs},
  langid = {english}
}


