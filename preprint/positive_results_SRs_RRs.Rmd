---
title             : "An excess of positive results: Comparing the standard Psychology literature with Registered Reports"
shorttitle        : "Positive Results in Standard vs Registered Reports"

author: 
  - name          : "Anne M. Scheel"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Den Dolech 1, Atlas 9.417, 5600 MB, Eindhoven, The Netherlands"
    email         : "a.m.scheel@tue.nl"
  - name          : "Mitchell Schijen"
    affiliation   : "1"
  - name          : "Daniël Lakens"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology"
    
note: 

author_note: 

abstract: > 
 When studies with positive results that support the tested hypotheses have a higher probability of being published than studies with negative results, the literature will give a distorted view of the evidence for scientific claims. Psychological scientists have been concerned about the degree of distortion in their literature due to publication bias and inflated Type-1 error rates. Registered Reports were developed with the goal to minimise such biases: In this new publication format, peer review and the decision to publish take place before the study results are known. We compared the results in the full population of published Registered Reports in Psychology ($N = 71$ as of November 2018) with a random sample of hypothesis-testing studies from the standard literature ($N = 152$) by searching 633 journals for the phrase ‘test* the hypothes*’ [replicating a method by @Fanelli2010]. Analysing the first hypothesis reported in each paper, we found $96\%$ positive results in standard reports, but only $44\%$ positive results in Registered Reports. The difference remained nearly as large when direct replications were excluded from the analysis ($96\%$ vs $50\%$ positive results). This large gap suggests that psychologists under-report negative results to an extent that threatens cumulative science. Although our study did not directly test the effectiveness of Registered Reports at reducing bias, these results show that the introduction of Registered Reports has led to a much larger proportion of negative results appearing in the published literature compared to standard reports.
  
keywords          : "Publication bias, Registered Reports, hypothesis testing"
#wordcount         : "6492"

header-includes:
  - \usepackage{float}
  - \usepackage{framed}
  - \usepackage{caption}
  - \usepackage{setspace}
  - \captionsetup[figure]{font={stretch=1, small}, skip=12pt}
  - \captionsetup[textbox]{name=Box,labelsep=period,labelfont=it}
  - \newfloat{textbox}{thp}{lop}
  - \floatname{textbox}{Box}


bibliography      : ["prr.bib","prr_software.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : no

lang              : "en-UK"
class             : "jou"
output            : papaja::apa6_pdf
---

```{r include = FALSE}
library("papaja")
library("bookdown")
library("rmarkdown")
library("knitr")
library("here")
library("TOSTER")
library("ggplot2")
library("stringr")
library("reshape2")
library("codebook") # not needed to knit the manuscript, but loaded here so it automatically gets cited at the end since it was used to create the codebook
library("rio") # not needed to knit the manuscript, but loaded here so it automatically gets cited at the end since it was used to create the codebook
```

If the scientific literature were a faithful representation of the research scientists conduct, a cumulative science would be a powerful tool to infer what is true about the world. 
When random error is the only threat to the accuracy of individual findings, aggregating across many findings allows inferences about the presence and size of effects with a certain reliability. 
But when published findings are systematically biased, cumulative science breaks down: 
Unlike random error, bias does not cancel out when aggregating across studies -- in the worst case it accumulates, leading us away from the truth rather than towards it.
Unfortunately there are good reasons to believe that the Psychology literature is not a faithful representation of all research psychologists conduct. 
For more than half a century, scientists have repeatedly noticed a suspiciously high "success" rate in Psychology:
Studying 362 empirical articles published in four Psychology journals in 1955/56, @Sterling1959 found that $97.28\%$ of the studies that used significance tests rejected the null hypothesis. 
A replication of this study performed on articles published in 1986/87 reported $95.56\%$ statistically significant results [@Sterling1995]. 
Similarly, a seminal study by @Fanelli2010 compared the literatures of 20 disciplines and found that $91.5\%$ of papers published in Psychology reported support for their first or main hypothesis, the highest estimate of all disciplines in the study. 
For these percentages to be a realistic representation of the research that psychologists perform, both statistical power and the proportion of true hypotheses (i.e., the prior probability that the null hypothesis is false) that are tested must exceed $90\%$. 
In other words: nearly all predictions researchers make must be correct, and either the studied effect sizes or the used sample sizes (given the same study design) must consistently be very high.

## A biased literature
@Sterling1959 already suspected a selection process behind the numbers he found: 
"(...) for psychological journals a policy exists under which the vast majority of published articles satisfy a minimum criterion of [statistical] significance" (p. 31). 
This selection process is one of several kinds of biases that will lead to an inflation of positive results in the literature. 
We can distinguish two broad categories of bias: "publication bias" and "questionable research practices". 
Publication bias describes publishing behaviours that give manuscripts that find support for their tested hypotheses a higher chance of being published than manuscripts that do not find support for their tested hypotheses. 
These include editors and reviewers selectively rejecting manuscripts with negative results ["reviewer bias", @Greenwald1975; @Mahoney1977] and researchers deciding not to submit studies with negative results for publication ["file-drawering"; @Rosenthal1979]. 
Questionable research practices (QRPs) describe research behaviours that make the evidence in favour of a certain conclusion look stronger than it is [typically, though not always, leading to an inflated type-I error rate, see also @Lakens2019b]. 
These include presenting unexpected results as having been predicted a priori [HARKing, short for "hypothesising after results are known"; @Kerr1998] and exploiting flexibility in data analysis to obtain statistically significant results ["*p*-hacking"; @Simmons2011]. 
Evidence for both categories of bias exist: 
Publication bias has been shown in peer review [@Mahoney1977; @Atkinson1982] and in longitudinal data from an NSF grant programme that found a file-drawering effect for studies with negative results [@Franco2014; @Franco2016]; and QRPs have been admitted by scientists in several survey studies [@John2012; @Fiedler2016; @Agnoli2017].

Some have argued that selecting for statistically significant results is defensible -- desirable, even -- because it weeds out low-quality research that would only pollute the literature [@Cleophas1999; see also @deWinter2013; and @vanAssen2014, for a critique]. 
How problematic selective publishing is in practice remains an empirical question: 
If most negative results that are currently missing from the literature are the result of immature ideas or poorly conducted studies, we should expect that a literature in which studies are selected based on their quality but not based on their results  would contain a similar proportion of positive results as the current one. 
But how many positive results would such an unbiased literature contain in reality? 
We set out to explore this question by comparing the rate of positive results in a sample from the current Psychology literature to a set of studies that were published in a new format created to minimise QRPs and publication bias: Registered Reports. 

## Methods to mitigate bias
An increasingly popular proposal to reduce bias is preregistration, where authors register a time-stamped protocol of their hypotheses, planned method, and analysis plan before data collection [for a historical overview, see @Wiseman2019]. 
Preregistration is thought to mitigate QRPs by preventing HARKing (hypotheses must be stated before the results are known) and by reducing the risk of *p*-hacking via restricted flexibility in data analysis. 
However, preregistration does not prevent file-drawering or reviewer bias and may thus be insufficient to fight publication bias [@Rasmussen2009; @Goldacre2016; but see @Kaplan2015]. 
A more effective safeguard against both publication bias and QRPs is promised by Registered Reports [@Chambers2013; @Nosek2014; @Chambers2015; @Jonas2016]. 
Registered Reports (RRs) are a new publication format with a restructured submission timeline: 
Before collecting data, authors submit a study protocol containing their hypotheses, planned procedures, and analysis pipeline (typically in the form of an Introduction and Method section) to a journal. 
The protocol undergoes peer review, and, if successful, receives "in-principle acceptance", meaning that the journal commits to publishing the final article following data collection regardless of the statistical significance of the results. 
The authors then collect and analyse the data and write up the final report. 
The final report undergoes another round of peer review, but this time only to ensure that the authors adhered to the registered plan and did not draw unjustified conclusions (and, if applicable, that the data pass pre-specified quality checks).
Registered Reports thus combine an antidote to QRPs (preregistration) with an antidote to publication bias, because studies are selected for publication before their results are known. 
Since its introduction at the journal *Cortex* in 2013, the format has rapidly gained popularity and is offered by 225 journals at the time of writing (http://cos.io/rr).

In addition to bias protection, Registered Reports promise high-quality research: Stage-1 peer review (pre data) increases the likelihood that methodological flaws and immature or misguided ideas will be spotted and fixed (or weeded out) before a study is conducted, and authors typically have to include outcome-neutral control conditions that allow verifying data quality once results are in (studies failing these quality checks may be rejected). 
Many journals offering Registered Reports also require that planned hypothesis tests are based on a power analysis that ensures a high probability of finding a statistically significant result if there is a true effect of the expected size (e.g., $90\%$ power for a given effect size of interest^[An overview of the requirements specified by each participating journal is available at https://docs.google.com/spreadsheets/d/1D4_k-8C_UENTRtbPzXfhjEyu3BfLxdOsn9j-otrO870]). 

Assuming a constant alpha level, the rate of positive results in a literature (i.e., the proportion of supported hypotheses among all tested hypotheses) is influenced by three factors: the proportion of true hypotheses among all tested hypotheses, statistical power, and bias. 
The Registered Reports format combines powerful safeguards against publication bias and QRPs with standards for research quality that are at least equal to ordinary peer review, and often include statistical power requirements that likely exceed those in the standard literature [see e.g., @Maxwell2004; @Szucs2017; @SingletonThorn2019]. 
Therefore, if the emerging Registered Reports literature in Psychology contains fewer positive results than the standard literature, the cause must be either the difference in bias or a lower proportion of true hypotheses tested in Registered Reports (or a combination of the two). 
At this time, we have good reasons to believe in a difference in bias, but less reason to believe in a difference in the proportion of true hypotheses (at least regarding original work, see below), which would make bias a more plausible explanation of a potential difference in the positive result rate. 
Considering the high standards for research quality in Registered Reports, a large difference in positive results between Registered Reports and the standard literature would also mean that publication bias is not a desirable filter for poorly conducted studies, but that we should worry about the high-quality negative results we are missing because of it.

## The current study
The goal of our study was to test if Registered Reports in Psychology show a lower positive result rate than articles published in the traditional way (henceforth referred to as "standard reports", SRs), and to estimate the size of this potential difference. 
We set out to replicate a study by @Fanelli2010 on a new sample of standard reports in Psychology and compared them to all published Registered Reports in Psychology. 
Fanelli searched for articles containing the phrase "test$^\ast$ the hypothes$^\ast$", drew a random sample of 150 articles per discipline, and for each of these coded if the first hypothesis mentioned in the abstract or full text had been supported or not. 
For standard reports we used the same sampling method (restricted to the Psychology discipline), and for Registered Reports we relied on a database curated by the Center for Open Science (COS). 
We chose this method because Fanelli's 2010 and 2012 studies (both use the same coding method) have been highly influential, and his method can easily be applied to a large set of studies. 
We additionally coded if studies were replications or original work because many published Registered Reports are replications. 
If replications are motivated by scepticism of the original results, the prior probability of hypotheses tested in these studies may be lower than in original studies, which could lead to a larger proportion of negative results regardless of bias.

In a recent commentary on benefits and challenges of open-science practices for early-career researchers, @Allen2019 conducted a similar investigation: 
They coded the proportion of null results in the 127 biomedical and Psychology Registered Reports listed in the COS database as of September 2018. 
We were not aware of their parallel efforts when we planned our study in September and October 2018. 
Allen and Mehler used a self-developed method to code the percentage of unsupported hypotheses in Registered Reports (counting all hypotheses in each paper) and found $60.5\%$ unsupported hypotheses across all included Registered Reports, $66\%$ for replication attempts, and $54.5\%$ for novel research. 
They compared these numbers to an estimate of 5--20$\%$ null results in the standard literature. 
This 5--20$\%$ estimate is based on data from @Fanelli2012, who coded only the first hypothesis of each paper [identical to @Fanelli2010], and @Cristea2018, who coded the percentage of statistically significant results in figures and tables of articles published in Nature, Science, and PNAS. 

A major advantage of our study is that it allows us to draw a more meaningful comparison between Registered Reports and the standard literature because we apply a previously used method [@Fanelli2010; @Fanelli2012] to both groups. 
In addition, we replicate @Fanelli2010 and provide data to evaluate his method: The search term "test$^\ast$ the hypothes$^\ast$" might introduce selection effects, meaning that results obtained this way may not generalise to hypothesis-testing studies that do not use this phrase. 
Therefore we also coded the phrases used to introduce hypotheses in Registered Reports, analysed how many of them would have been detected with Fanelli's search term, and compiled a list of alternative search terms to test the generalisability of Fanelli's results in the future. 
Finally, we share a rich dataset containing the exact quotes of hypotheses and conclusions on which we based our judgements, as well as detailed descriptions of our sampling and coding procedure (see Appendix). 
This allows others to verify (or contest) our results and may provide an interesting resource for future meta-scientific research.

# Methods
After conducting a pilot to test the planned procedure, we preregistered our study (https://osf.io/s8e97). 
Methods and analyses described here were preregistered unless otherwise noted. 
A detailed comparison of our preregistration and the eventual procedure is provided in the supplement. 
We report how we determined our sample size, all data exclusions, and all measures in the study.

## Sample
We used the same method as @Fanelli2010 to obtain a new sample of standard reports, but restricted year of publication to 2013-2018 to match the sample to the Registered Reports population. 
We excluded papers if they were incomplete, unpublished, or retracted articles (e.g., meeting abstracts, study protocols without results), if they did not test a hypothesis, or if they contained insufficient information to reach a coding decision. 
An overview of the sampling process and all exclusions is shown in Figure\ \@ref(fig:sampling). 

For standard reports we downloaded a current version of the Essential Science Indicators (ESI) database (retrieved on 4th December 2018) and used Web of Science to search for articles published between 2013 and 2018 with a Boolean search query containing the phrase "test$^\ast$ the hypothes$^\ast$" and the ISSNs of all 633 journals listed in the ESI Psychiatry/Psychology category. 
Using the same sample size as @Fanelli2010, we randomly selected 150 papers from the 1919 search results using the sample() function in R and the seed "20190120" (seed was not preregistered, but no other seeds were tried).
We initially excluded eight papers and replaced them (decision to replace excluded papers was not preregistered) through the same random sampling procedure until 150 studies were found that met our criteria, but later found that two papers had been excluded erroneously, leading to a final sample size of 152 (see Fig.\ \@ref(fig:sampling)). 

(ref:sampling) Sampling process and exclusions for standard reports and Registered Reports. Standard reports were accidentally oversampled: We initially excluded 8 papers and only after replacing them found that two had been excluded erroneously. "Preregistered": study had been preregistered but was not a full RR; "results-blind review": article had undergone results-blind peer review but was not a full RR (authors knew results before first submission); "ambiguous": four of these had been treated as Registered Reports but used pre-existing data to which the authors had access before conducting their analyses, one had no explicit signs of an RR except for a 2.5-year delay between submission and acceptance (we chose to exclude these cases to be conservative).

```{r sampling, echo=FALSE, warning=FALSE, fig.cap = "(ref:sampling)", out.width = "\\textwidth", fig.env="figure*"}

# Important additional chunk option when using journal mode: fig.env="figure*"

knitr::include_graphics("sampling_process_flowchart.png")
```

For Registered Reports we aimed to include all published Registered Reports in the field of Psychology that tested at least one hypothesis, regardless of whether or not they used the phrase "test$^\ast$ the hypothes$^\ast$". 
We downloaded a database of published Registered Reports curated by the Center for Open Science^[https://www.zotero.org/groups/479248/osf/items/collectionKey/KEJP68G9] (retrieved on 19th November 2018), and excluded papers published in journals that were listed in categories other than "Psychiatry/Psychology" or "Multidisciplinary" in the ESI. 
Note that the decision to focus only on the Psychiatry/Psychology category meant excluding 13 Registered Reports published in *Cortex* because the ESI counts this journal towards the separate category "Neuroscience and Behavior". 
Papers published in multidisciplinary journals and in journals not included in the ESI (e.g., *Royal Society Open Science*) were hand-coded by AS. 
This deviates from our preregistration insofar as we had not specified how discipline membership would be determined.

Following these exclusions, we verified the Registered Reports status of all remaining papers in our sample. 
Papers were counted as Registered Reports if they were labelled as such by the journal itself and the journal submission guidelines made it clear that these submissions had been reviewed and received in-principle acceptance before the data collection (or analyses) of all studies in the paper had been conducted (in accordance with https://cos.io/rr). 
For papers not clearly labelled as Registered Reports, we consulted relevant editorial publications (e.g., for special issues) or contacted the respective editors directly. 
Of the 151 entries in the COS Registered Reports database, 55 were excluded because they belonged to a non-Psychology discipline, 12 because we could not verify that they were Registered Reports, and 13 because they did not test hypotheses or contained insufficient information, leaving 71 Registered Reports for the final analysis (see Fig.\ \@ref(fig:sampling)). 
Note that we excluded all eight "Registered Replication Reports" [RRRs; @Simons2014; @Simons2018] in our sample because this format explicitly focusses on effect size estimation and not hypothesis testing [@RRRwebsite, decision was not preregistered].

## Measures and coding procedure
The main dependent variable was whether the first hypothesis was supported or not, as reported by the authors.
We tried to follow Fanelli's [-@Fanelli2010] coding procedure as closely as possible, which he describes as follows: 

> By examining the abstract and/or full- text, it was determined whether the authors of each paper had concluded to have found a positive (full or partial) or negative (null or negative) support. 
> If more than one hypothesis was being tested, only the first one to appear in the text was considered. 
> We excluded meeting abstracts and papers that either did not test a hypothesis or for which we lacked sufficient information to determine the outcome. (p. 8) 

Like @Fanelli2010, we coded hypotheses as having received "support", "partial support", or "no support", which was recoded into a binary "support" (full or partial) vs "no support" variable for the analysis. 
Coding disagreements between full and partial support were deemed minor since they would not affect the final results. Thus, only disagreements affecting the binary support/no support classification were treated as major and resolved through discussion.

Before preregistering our study, we conducted a pilot to assess if we could employ Fanelli's method successfully.
Originally we had planned to first reproduce his results on the same sample of Psychiatry/Psychology articles used in @Fanelli2010. 
Unfortunately the author refused to share the original data (or even a list of the coded articles) with us. 
Instead, we received an excerpt which contained data for 11 records from the original sample, but no reference information of the coded articles (personal communication, 5th October 2018). 
We were able to find these 11 articles based on the hypothesis quotes that had been coded, and used them as a pilot sample along with 10 randomly selected Registered Reports. 
MS and AS independently coded all 21 pilot articles with only one major disagreement in each group. 
In the standard reports group, this disagreement was also the only case of major disagreement with Fanelli's original coding, which we deemed satisfying to proceed. 

```{r include=FALSE}
source(here("analysis", "01_interrater_agreement.R"))
```


Based on our experiences during the pilot, we added one coding criterion: 
If the first hypothesis mentioned in a paper was not explicitly tested but subsequently divided into sub-hypotheses that were tested, we would code the first *tested* hypothesis rather than the first hypothesis mentioned in the text. 
In Registered Reports we coded the first preregistered hypothesis, thus excluding unregistered pilot studies. 
MS coded all papers in the sample, AS double-coded all papers MS had found difficult to code or could not code ($`r sum(included$coded_by_AS & included$is_RR==1)`$ RRs and $`r sum(included$coded_by_AS & included$is_RR==0)`$ SRs). 
Only `r support.disagreement.major` disagreements were major (Cohen's kappa = `r printp(kappa.support)`) and subsequently resolved by discussion; `r support.disagreement.minor` were minor (disagreement between "support" and "partial support"). 
We had preregistered that AS would additionally code a random subset of both groups, but decided against it because the number of double-coded papers seemed sufficient after double-coding only the difficult cases.

### Hypothesis introductions
Selecting papers that use the phrase "test$^\ast$ the hypothes$^\ast$" might yield different results than alternative search phrases. 
Getting a better overview of "natural" descriptions of hypotheses would be useful for future investigations of the generalisability of Fanelli's [-@Fanelli2010] results and could inspire new research questions. 
We therefore extracted the phrase used to introduce the hypothesis from the coded hypothesis quotes for all Registered Reports and tried to identify clusters of similar expressions which may be used to create alternative search phrases. 

### Replication status
We also wanted to code if a study was a replication of a previously published one: 
We expected a much larger proportion of Registered Reports to be direct replications, many of which may have been motivated by scepticism of the original study. 
A lower positive result rate in Registered Reports could then be an effect of failed replications rather than an effect of safeguards against QRPs and publication bias. 
After an initial coding attempt with ill-defined coding criteria had led to too many disagreements (described further in the Appendix), we developed the following strategy (not pre-registered): 
We searched the full texts of all papers for the string "replic$^\ast$" [cf. @Makel2012; @Kohler2019; @Mueller-Langer2019; @Pridemore2018] and, for papers that did contain it, determined whether the coded hypothesis was a close replication with the goal to verify a previously published result. 
Conceptual replications and internal replications (replication of a study in the same paper) were not counted as replications in this narrow sense, since both are more likely to be motivated by the goal to build on previous work than by scepticism.
AS coded all papers, DL double-coded `r rep.RR.coded.DL` Registered Reports ($`r printnum(rep.RR.coded.DL/sum(included$is_RR)*100)` \%$) and `r rep.SR.coded.DL` standard reports ($`r printnum(rep.SR.coded.DL/sum(included$is_RR==0)*100)` \%$). 
There were `r rep.disagreements` disagreements (Cohen's kappa = `r printp(kappa.rep)`), all were resolved by discussion. 

### Additional measures
All additional measures we collected but have not described thus far were either auxiliary variables to facilitate the coding process or earlier versions of the variables discussed above. 
All of these are documented in the Appendix and in our shared dataset and codebook.


## Analysis
We planned to test our hypothesis in the following way (quoting directly from our preregistration, [https://osf.io/sy927](https://osf.io/sy927), point 9): 

> A one-sided proportion test with an alpha level of $5\%$ will be performed to test whether the positive result rate (full or partial support) of Registered Reports in psychology is statistically lower than the positive result rate of conventional reports^[We later changed the term to "standard reports".] in psychology. 
> In addition to testing if there is a statistically significant difference between RRs and conventional reports, we will test if the difference is smaller than our smallest effect size of interest using an equivalence test for proportion tests with an alpha level of $5\%$ [@Lakens2018a].
> We determined our smallest effect size of interest to be the difference between the positive result rate in psychology ($91.5\%$) and the positive result rate in general social sciences ($85.5\%$) as reported by @Fanelli2010, i.e. a difference of $91.5\% - 85.5\% = 6\%$. 
> The rationale for choosing general social sciences as a comparison is that this discipline had the lowest positive result rate amongst the "soft" sciences [@Fanelli2010]. 
> The exact percentage for general social sciences was extracted from Figure 1 in @Fanelli2010 using the software WebPlotDigitizer [@Rohatgi2018].

We would accept our hypothesis that Registered Reports have a lower positive result rate than standard reports if we found a negative difference between Registered Reports and standard reports that was significantly different from 0 *and* not statistically equivalent to a range from $-6\%$ to $+6\%$ (both at $\alpha = 5\%$). 


# Results
```{r include=FALSE}
source(here("analysis", "02_quantitative_analyses.R"))
```

## Confirmatory Analyses

`r n.support.SR` out of `r n.SR` standard reports and `r n.support.RR` out of `r n.RR` Registered Reports had positive results, meaning that the positive result rate was $`r printnum(prop.support.SR*100)` \%$ for standard reports ($95 \%$ CI [`r printnum(min(SR.binom$conf.int)*100)`, `r printnum(max(SR.binom$conf.int)*100)`]) and $`r printnum(prop.support.RR*100)` \%$ for Registered Reports ($95 \%$ CI [`r printnum(min(RR.binom$conf.int)*100)`, `r printnum(max(RR.binom$conf.int)*100)`]; see Fig.\ \@ref(fig:mainplot)). 
The preregistered one-sided proportions test with an alpha level of $5\%$ showed that this difference of $-`r printnum((prop.support.SR-prop.support.RR)*100)` \%$ was statistically significant, $\chi^2 = `r printnum(proptestresult$statistic)`$, $p `r printp(proptestresult$p.value)`$.
Unsurprisingly, the difference was not statistically equivalent to a range between $`r -SESOI*100` \%$ and $`r SESOI*100` \%$ at $\alpha = 5\%$, $z = `r printnum(min(abs(tostresult$TOST_z1), abs(tostresult$TOST_z2)))`$, $p `r printp(max(tostresult$TOST_p1, tostresult$TOST_p2))`$, meaning that we cannot reject differences more extreme than $`r SESOI*100`\%$. 
We thus accept our hypothesis that the positive result rate in Registered Reports is lower than in standard reports.

(ref:mainplot) Positive result rates for standard reports and Registered Reports. Error bars indicate $95 \%$ confidence intervals around the observed positive result rate.

```{r mainplot, echo=FALSE, warning=FALSE, fig.height=5, fig.cap="(ref:mainplot)"}
#mainplot

#####
# Important alternative code and settings when using journal mode:
# 1. set fig.height=5 in chunk options (it's 4 in manuscript mode)
# 2. use the plot code below instead of `mainplot`
#    (otherwise font will be too small)!
#
ggplot(mainplot.df, aes(x = is_RR, y = value, fill = support)) +
                    geom_bar(stat = "identity",
                             width = 0.5) +
                    scale_fill_manual(values= c("#bcbddc", "#756bb1"),
                                      name="first hypothesis") +
                    annotate("text", label = paste("N =", n.SR),
                             x = 1, y = 105, size = 5) +
                    annotate("text", label = paste("N =", n.RR),
                             x = 2, y = 105, size = 5) +
                    geom_errorbar(aes(ymin = lower, ymax = upper),
                                  width = 0.05, size = 0.5) +
                    scale_x_discrete(breaks = waiver(),
                                     labels = c("SR" = "Standard\nReports",
                                                "RR" = "Registered\nReports"),
                                     name = NULL)+
                    scale_y_continuous(lim=c(0,110), breaks = c(seq(0, 100, 10)),
                                       minor_breaks = c(seq(0, 100, 5)),
                                       name = "% of papers", expand = c(0, 0))+
                    theme_minimal(base_size = 20)+
                    theme(panel.grid.major.x = element_blank(),
                          panel.grid.minor.x = element_blank(),
                          legend.margin = margin(0,0,0,-5.5)) +
                    coord_fixed(ratio=0.03)
```


## Exploratory Analyses
As described in the Method section, we only classified direct replications of previously published work as replications. 
This means that our non-replication category also contains some conceptual replications and "internal" replications (where original and replication are published in the same paper). 
For ease of communication we will nonetheless refer to this category as "original" studies.
As expected, direct replications were much more common among Registered Reports than standard reports: `r n.rep.RR` out of `r n.RR` Registered Reports ($`r printnum(prop.rep.RR*100)`\%$), but only `r n.rep.SR` out of `r n.SR` standard reports ($`r printnum(prop.rep.SR*100)`\%$) were classified as direct replications of previously published work. 
However, this difference cannot account for the stark overall difference between standard reports and Registered Reports described above: 
Although replication Registered Reports in our sample indeed had a lower positive result rate than original Registered Reports (see Table 1), the difference between original standard reports and original Registered Reports, $`r printnum((prop.orig.support.SR-prop.orig.support.RR)*100)`\%$, was still significantly different from 0 ($\chi^2 = `r printnum(origproptest$statistic)`$, $p `r printp(origproptest$p.value)`$) and not statistically equivalent to a range between $`r -SESOI*100`\%$ and $`r SESOI*100`\%$ ($z = `r printnum(min(abs(reptost$TOST_z1), abs(reptost$TOST_z2)))`$, $p `r printp(max(reptost$TOST_p1, reptost$TOST_p2))`$), both at $\alpha = 5\%$.

(ref:table1) Positive results in original studies vs replication studies

```{r echo = FALSE, results = 'asis'}
apa_table(
  rep.table
  , digits = c(0, 0, 0, 2, 0, 0, 0, 2, 0)
  , col.names = c("", "n", "supported", "\\%", "95\\% CI", "n", "supported", "\\%", "95\\% CI")
  , align = c("l", rep("r", 8))
  , caption = "(ref:table1)"
  , note = "SRs = standard reports, RRs = Registered Reports"
  , added_stub_head = "Variables"
  , col_spanners = list(`original studies` = c(2, 5), `replication studies` = c(6, 9))
)
```

Since our standard-reports sample represents a direct replication of @Fanelli2010 for the discipline Psychiatry & Psychology, another interesting question to ask is how our results compare to Fanelli's.
The difference between the positive result rate for standard reports in our sample ($`r printnum(prop.support.SR*100)`\%$) and Fanelli's ($`r printnum(prop.support.Fanelli*100)`\%$) is $`r printnum((prop.support.SR-prop.support.Fanelli)*100)`\%$. This difference is not significantly different from 0 in a two-sided proportions test ($\chi^2 = `r printnum(Fanelliproptest$statistic)`$, $p= `r printp(Fanelliproptest$p.value)`$) but also not statistically equivalent to a range between $`r -SESOI*100`\%$ and $`r SESOI*100`\%$ ($z = `r printnum(min(abs(Fanellitost$TOST_z1), abs(Fanellitost$TOST_z2)))`$, $p= `r printp(max(Fanellitost$TOST_p1, Fanellitost$TOST_p2))`$), both at $\alpha = 5\%$. 
In other words, we can neither reject the hypothesis that the positive result rates of the two populations are the same, nor that there is a difference of at least $\pm `r SESOI*100`\%$ between them. 
The data are inconclusive.


```{r include=FALSE}
source(here("analysis", "03_hypothesis_introduction_analyses.R"))
```
Finally, we analysed the language that was used to introduce or refer to hypotheses in Registered Reports.
We were interested in whether the search phrase "test$^\ast$ the hypothes$^\ast$" used by Fanelli captures the way researchers write about hypothesis tests reasonably well. The answer is a resounding "no":
Searching the abstracts, titles, and keywords of the Registered Reports sample showed that only `r testthehypothes.RR`/`r n.RR` Registered Reports would have been detected with this search phrase. 
To get an overview of analogous hypothesis-introduction phrases researchers used in Registered Reports, we stripped the hypothesis quotes of Registered Reports from all content-specific information and extracted "minimal" phrases that most distinctively indicated that a hypothesis was being described.
For example, from the hypothesis quote "(f)or Study 1, we predicted that participants reading about academic (vs. social) behaviors would show a better anagram performance" we extracted the hypothesis-introduction phrase "predicted that". 

For the majority of Registered Reports (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==1))`), we identified one hypothesis-introduction phrase; the remaining ones used two (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==2))` RRs), three (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==3))` RRs), or four (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==4))` RR) different phrases or had no identifiable hypothesis introduction (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==0))` RR).
In this total set of `r nrow(intros.long)` hypothesis introductions, we found `r length(uniquephrases)` unique phrases showing substantial linguistic variation (see Tables 2 and 3).
To condense the information, we listed all unique word stems (e.g., the word stem "hypothes$^\ast$" captures the words "hypothesis", "hypotheses", "hypothesize", "hypothesized", and so on) and analysed their frequency among all hypothesis introductions.
Excluding words that are common but too unspecific by themselves, such as "that", "to", or "whether", the five most frequent word stems were "hypothes$^\ast$" (`r wordoccurrences.df[1,2]` occurrences), "replicat$^\ast$" (`r wordoccurrences.df[2,2]`), "test$^\ast$" (`r wordoccurrences.df[3,2]`), "examine$^\ast$" (`r wordoccurrences.df[4,2]`), and "predict$^\ast$" (`r wordoccurrences.df[5,2]`). 
Clearly "test$^\ast$" and "hypothes$^\ast$" are quite popular, yet they co-occurred only `r test.and.hypothes` times and more than half of all hypothesis introductions (`r nrow(intros.long)-test.or.hypothes`/`r nrow(intros.long)`) contained neither word. Interestingly, the frequency of these two words differed between original studies and direct replications: 30 out of 43 ($`r printnum(30/43*100)`\%$) hypothesis introductions in original Registered Reports contained either "test$^\ast$" or "hypothes$^\ast$" or both, but the same was true for only 16 out of 54 ($`r printnum(16/54*100)`\%$) hypothesis introductions in direct replication Registered Reports.

We noticed that direct replication Registered Reports generally tended to use different language to describe their hypothesis. As the high frequency of the word stem "replicat$^\ast$" suggests, these studies were often not framed as *tests* of a previously tested hypothesis, but as attempts to repeat a previously conducted procedure. 
Authors thus seemed to have focussed more on the goal to replicate a previous finding than to test a hypothesis.

Tables 2 and 3 list all unique hypothesis introductions and their frequency in original Registered Reports and direct replication Registered Reports, respectively, grouped by the five most frequent word stems ("hypothes$^\ast$", "replicat$^\ast$", "test$^\ast$", "examine$^\ast$", "predict$^\ast$"). 
Using five as a cut-off value is an arbitrary decision, but we believe that it strikes a reasonable balance between condensing the information and doing the variance of the data justice. 

(ref:table2) Hypothesis introduction phrases in original Registered Reports (testing new hypotheses)

```{r echo = FALSE, results = 'asis'}
apa_table(
  phrasetable.orig
  , digits = c(0, 0, 0, 0, 0)
  , col.names = c("core word(s)", "introduction phrase", "abstract", 
                  "full text", "total")
  , align = c("l", "l", rep("r", 3))
  , caption = "(ref:table2)"
  , note = "Table contains 44 hypothesis introduction phrases from 30 Registered Reports: 19 papers contributed one phrase each, nine papers contributed two each, one contributed three, and one contributed four."
  , added_stub_head = "Variables"
  , col_spanners = list(`source` = c(3, 5))
  , midrules = c(10, 16, 22, 23, 28, 32)
  , landscape = FALSE
  , font_size = "footnotesize"
  , escape = TRUE
)
```

(ref:table3) Hypothesis introduction phrases in direct replication Registered Reports (testing previously studied hypotheses)

```{r echo = FALSE, results = 'asis'}
# Important alternative settings when using journal mode: font_size = "footnotesize"
# (it's scriptsize in manuscript mode)
apa_table(
  phrasetable.rep
  , digits = c(0, 0, 0, 0, 0, 0, 0)
  , col.names = c("", "core word(s)", "introduction phrase", "abstract", 
                  "full text", "total", "")
  , align = c("l", "l", "l", rep("r", 4))
  , caption = "(ref:table3)"
  , note = "Table contains 53 hypothesis introduction phrases from 40 Registered Reports. One additional RR had no identifiable hypothesis introduction. Thirty papers contributed one phrase each, seven contributed two each, and three contributed three each."
  , added_stub_head = "Variables"
  , col_spanners = list(`source` = c(4, 6))
  , midrules = c(6, 10, 11, 12, 29, 30, 34, 35, 36)
  , landscape = FALSE
  , font_size = "footnotesize"
  , escape = TRUE
)
```

It is important to keep in mind that not all hypotheses could be coded from the abstract: For `r n.hypintro.fulltext.only` Registered Reports, the hypothesis introduction phrases analysed above came only from the full text, which means that search terms extracted from them may not be useful in literature searches focussed only on titles, abstracts, and keywords. 
Therefore we additionally tested how many of the Registered Reports would have been detected in a regular search using our five most frequent word stems. 
We searched titles, abstracts, and keywords for "hypothes$^\ast$" OR "replicat$^\ast$" OR "test$^\ast$" OR "examine$^\ast$" OR "predict$^\ast$" and found that $`r n.hypintros.fivewords.RR`/`r n.RR`$ Registered Reports ($`r printnum(n.hypintros.fivewords.RR/n.RR*100)` \%$) would have been detected this way. 
We do not know how well these search terms represent the population of hypothesis-testing studies in Psychology, but a structured investigation of this question would be very useful for future meta-research.

# Discussion
We examined the proportion of Psychology articles that find support for their first tested hypothesis and discovered a large difference ($`r printnum(prop.support.SR*100)` \%$ vs $`r printnum(prop.support.RR*100)` \%$) between a random sample of standard reports and the full population of Registered Reports (at the time of data analysis). 
More than half of the analysed hypothesis tests in Registered Reports were direct replications of previous work, but the difference between standard reports and Registered Reports was still large when direct replications were excluded from the analysis ($`r printnum(prop.orig.support.SR*100)` \%$ vs $`r printnum(prop.orig.support.RR*100)` \%$). 
The introduction of Registered Reports has clearly led to a much larger proportion of null results appearing in the published literature compared to standard reports. 

The positive result rate we found in standard reports ($`r printnum(prop.support.SR*100)` \%$) is slightly higher than the $91.5\%$ reported by @Fanelli2010, although this difference was not statistically significant. 
Our replication in a more recent sample of the Psychology literature thus yielded a comparably high estimate of supported hypotheses, but we cannot rule out that the positive result rate in the population has increased since 2010 [cf. @Fanelli2012].
Furthermore, our estimate of the positive result rate for Registered Reports ($`r printnum(prop.support.RR*100)` \%$) is comparable to the $39.5\%$ reported by @Allen2019, despite some differences in method and studied population.

To explain the $`r printnum((prop.support.SR - prop.support.RR)*100)` \%$ gap between standard reports and Registered Reports, we must assume some combination of differences in bias, statistical power, or the proportion of true hypotheses researchers choose to examine. 
Figure\ \@ref(fig:powerbaserate) visualises the combinations of statistical power and proportion of true hypotheses that would produce the observed positive result rates if the literature were completely unbiased. 
For example, assuming no publication bias and no QRPs, even if *all* hypotheses authors of standard reports tested were true, their study designs would need to have more than $90\%$ power for the true effect size. 
This is highly unlikely, meaning that the standard literature is unlikely to reflect reality. 
As we noted above, there is good reason to assume that methodological rigour and statistical power in Registered Reports are as high as in standard reports or higher, leaving the rate of true hypotheses and bias as remaining explanations.

(ref:powerbaserate) Combinations of the proportion of true hypotheses and statistical power that would produce the observed positive result rates given $\alpha = 5 \%$ and no bias. Shaded areas indicate $95\%$ confidence intervals. SRs = standard reports, RRs = Registered Reports. The curve for all SRs (i.e, including replications; $`r printnum(prop.support.SR*100)` \%$ positive results, $N = `r n.SR`$) is not shown because it is almost identical to the one for original SRs. Plotted values were calculated using the equation $PRR = \alpha*(1-t) + (1-\beta)*t$; with $PRR =$ positive result rate, $\alpha =$ probability of obtaining a positive result when testing a false hypothesis (here fixed at .05), $1-\beta =$ probability of obtaining a positive result when testing a true hypothesis (power), and $t =$ proportion of true hypotheses; and solving for $t$ and $1-\beta$, respectively (with the simplifying assumption that all studies in one group have the same power).

```{r powerbaserate, echo=FALSE, fig.width=5, fig.height=3, fig.align='center', warning=FALSE, fig.cap="(ref:powerbaserate)", out.width = "0.6\\textwidth", fig.env="figure*"}

# Important altrnative chunk options when using journal mode: 
# 1. use out.width="0.6\\textwidth" (it's 0.8 in manuscript mode)
# 2. set fig.env="figure*"

source(here("analysis", "04_power_baserate_plot.R"))
power.baserate.plot
```



It is a-priori plausible that Registered Reports are currently used for a population of hypotheses that are less likely to be true: 
For example, authors may use the format strategically for studies they expect to yield negative results (which would be difficult to publish otherwise). 
However, assuming over $90\%$ true hypotheses in the standard literature is neither realistic, nor would it be desirable for a science that wants to advance knowledge beyond trivial facts. 
We thus believe that this factor alone is not sufficient to explain the gap between the positive result rates in Registered Reports and standard reports. 
Rather, the numbers strongly suggest a reduction of publication bias and/or Type-1 error inflation in the Registered Reports literature.

## Limitations
We compared hypotheses tested in Registered Reports with hypotheses tested in standard reports. 
Because hypotheses, authors, and editors were not randomly assigned to each publication format, we cannot draw firm conclusions about the causes that led to a difference in the proportion of supported hypotheses. 
Although it seems plausible that selective reporting and QRPs are reduced in Registered Reports, we do not know by how much, nor if this reduction would be of comparable size in a randomised experiment. 
As mentioned above, it is a-priori plausible that the Registered Reports format is used selectively for particularly risky hypotheses. 
This means that the proportion of true hypotheses in Registered Reports does not necessarily generalise to the entire population of hypotheses that are tested in Psychology. 
It is also important to note that our results do not warrant the conclusion that Registered Reports are effective at reducing all forms of bias. 
Authors self-select to submit Registered Reports, and the format may be particularly popular among those who try to minimise the risk of inflated error rates regardless of the report format they use. 
This would lead to less bias in the Registered Reports literature even if the format's safeguards against certain QRPs were actually ineffective. 

A second limitation of the current study [and of @Fanelli2010] is that standard reports were selected using the search term "test$^\ast$ the hypothes$^\ast$". 
As our results show, this phrase was virtually absent in the Registered Report population. 
The wide variety of ways to introduce a hypothesis we observed in Registered Reports suggests that a search for "test$^\ast$ the hypothes$^\ast$" will miss most of the hypothesis-testing studies in the psychological literature. 
Results based on this search phrase may thus not generalise to all published studies. 
For example, it is possible that authors are more likely to describe their research explicitly as a hypothesis test when they found positive results, but prefer more vague language for unsupported hypotheses (e.g., "we examined the role of ..."). 
If this were true, using other strategies to select standard reports might yield lower estimates for the positive result rate. 
However, this does not seem to be the case: 
Studies using different selection criteria for articles and hypotheses have found very similar rates of supported hypotheses. 
For example, the positive result rates in @Sterling1959, @Sterling1995, and the original studies included in the Reproducibility Project: Psychology [@OSC2015] were $97.28\%$, $95.56\%$, and $97\%$, respectively. 
@Motyl2017 report $89.17\%$ and $92.01\%$ significant results for "critical" hypothesis tests in papers published in 2003-2004 and 2013-2014, respectively. 
Therefore, although the search term used to find standard reports might limit the generalisability of our results, this search strategy seems to yield comparable estimates as the selection strategies used in different studies. 

A final limitation is the decision to code only the first reported hypothesis. 
The first hypothesis test may not be representative for all hypothesis tests reported in a paper, and the order of reporting may differ between standard reports and Registered Reports. 
Perhaps Registered-Report authors are more likely to present their hypotheses in "chronological" order, whereas standard-report authors tend to rearrange the order in which hypotheses are reported based on their outcomes, and present supported hypotheses first. 
Here again, the converging estimates from the four studies cited above (none of which use the first-hypothesis rule) make it seem unlikely that our result is an artefact of this decision. 
Regardless of which hypothesis one chooses to analyse across a set of papers -- the first, the last, or the "critical" one -- the positive result rate turns out to be higher than what can be expected based on realistic estimates of the proportion of true hypotheses researchers study and the statistical power of their tests. 

## Conclusion
Our study presents a systematic comparison of positive results in Registered Reports and the standard literature. The much lower positive result rate in Registered Reports compared to standard reports suggests that an unbiased literature would look very different from the existing body of published research. Standard publication formats seem to lead psychological scientists to miss out on many negative results from high-quality studies, which are available in the Registered Reports literature. The absence of negative results is a serious threat to a cumulative science. In 1959, Sterling asked: "What credence can then be given to inferences drawn from statistical tests of $H_0$ if the reader is not aware of all experimental outcomes of a kind?" The amount of experimental outcomes missing from the standard literature appears to be so large that not much credence may be left. In contrast, Registered Reports have clearly led to a much larger proportion of negative results appearing in the literature -- and may be one solution to a more credible scientific record.


```{r include=FALSE}
r_refs(file = "prr_software.bib")
my_citation <- cite_r(file = "prr_software.bib")
```

## Disclosures
### Data, materials, and online resources
The data and code necessary to reproduce all analyses reported here, as well as the [Appendix](https://osf.io/qw798/), the [preregistration document](https://osf.io/sy927/) and additional supplementary files, are available at <https://osf.io/dbhgr>. The manuscript including figures and statistical analyses, the [Appendix](https://osf.io/qw798/), and the [codebook](https://osf.io/6jrkz/) available in the supplement were created using RStudio [1.2.5019, @RStudioTeam2019] and `r my_citation`.

### Conflicts of Interest
The authors declare that they have no conflicts of interest with respect to the authorship or the publication of this article.

### Author Contributions
Conceptualisation: A.S. & D.L.; data curation, formal analysis, and software: A.S. & M.S.; investigation, methodology, and validation: A.S., M.S., & D.L; supervision: A.S & D.L.; visualisation and writing---original draft: A.S; writing---review and editing: A.S., M.S., & D.L.

### Acknowledgements
This work was funded by VIDI grant 452-17-013. We thank Chris Chambers, Emma Henderson, Leo Tiokhin, and Stuart Ritchie for valuable comments that helped improve this manuscript.


# References


\setlength{\parindent}{-0.2in}
\setlength{\leftskip}{0.2in}

