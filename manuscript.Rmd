---
title             : "Positive Results in Registered Reports"
shorttitle        : "Positive Results in Registered Reports"

author: 
  - name          : "Anne Scheel"
    affiliation   : "1"
    corresponding : yes    # Define only one corresponding author
    address       : "Den Dolech 1, Atlas 9.417, 5600 MB, Eindhoven, The Netherlands"
    email         : "a.m.scheel@tue.nl"
  - name          : "Mitchell Schijen"
    affiliation   : "1"
  - name          : "DaniÃ«l Lakens"
    affiliation   : "1"

affiliation:
  - id            : "1"
    institution   : "Eindhoven University of Technology"
    
note: "\\clearpage"

author_note: 

abstract: >
  XXXXXXXXXXXXXXXXXXX
  
keywords          : "keyword 1, keyword 2, keyword 3"
wordcount         : "XXX"

header-includes:
  - \usepackage{float}
  - \usepackage{framed}
  - \usepackage{caption}
  - \usepackage{setspace}
  - \captionsetup[textbox]{name=Box,labelsep=period,labelfont=it}
  - \newfloat{textbox}{thp}{lop}
  - \floatname{textbox}{Box}

bibliography      : ["prr.bib","prr_software.bib"]

figsintext        : yes
figurelist        : no
tablelist         : no
footnotelist      : no
lineno            : yes

lang              : "en-UK"
class             : "man"
output            : papaja::apa6_pdf
---

```{r include = FALSE}
library("papaja")
library("bookdown")
library("rmarkdown")
library("knitr")
library("TOSTER")
library("purrr")
library("lattice")
library("ggplot2")
library("dplyr")
library("stringr")
library("reshape2")
```

If the scientific literature were a faithful representation of the research scientists conduct, a cumulative science would be a powerful tool to infer what is true about the world. 
When random error is the only threat to the accuracy of individual findings, aggregating across many findings allows inferences about the presence and size of effects with a certain reliability. 
However, when published findings are systematically biased, cumulative science breaks down: 
Unlike random error, bias does not cancel out when aggregating across studies -- in the worst case it accumulates, leading us away from the truth rather than towards it.
Unfortunately there are good reasons to believe that the psychology literature is not a faithful representation of all research psychologists conduct. 
For more than half a century, scientists have repeatedly noticed a suspiciously high 'success' rate in psychology:
Studying 362 empirical articles published in four psychology journals in 1955/56, @Sterling1959 found that a whopping $97.28\%$ of the studies that used significance tests rejected the null hypothesis. 
A replication of this study performed on articles published in 1986/87 reported $95.56\%$ statistically significant results [@Sterling1995]. 
Similarly, a seminal study by @Fanelli2010 compared the literatures of 20 disciplines and found that $91.5\%$ of papers published in psychology reported support for their first or main hypothesis, the highest number of all disciplines in the study. 
For these percentages to be a realistic representation of the research that psychologists perform, both statistical power and the proportion of true hypotheses (i.e., the prior probability that the null hypothesis is false) that are tested must exceed $90\%$. 
In other words: nearly all predictions researchers make must be correct, and either the studied effect sizes or the used sample sizes must consistently be very high (given the same study design).

## A biased literature
@Sterling1959 already suspected a selection process behind the numbers he found: 
"(...) for psychological journals a policy exists under which the vast majority of published articles satisfy a minimum criterion of [statistical] significance" (p. 31). 
This selection process is one of several kinds of biases that will lead to an inflation of positive results in the literature. 
We can distinguish two broad categories of bias: "publication bias" and "questionable research practices". 
Publication bias describes publishing behaviours that give manuscripts that find support for their tested hypotheses a higher chance of being published than manuscripts that do not find support for their tested hypotheses. 
These include editors and reviewers selectively rejecting manuscripts with negative results ["reviewer bias", @Mahoney1977; @Greenwald1975] and researchers deciding not to submit studies with negative results for publication ["file-drawering"; @Rosenthal1979]. 
Questionable research practices (QRPs) describe research behaviours that make the evidence in favour of a certain conclusion look stronger than it is [typically, though not always, leading to an inflated type-I error rate, see also @Lakens2019b]. 
These include presenting unexpected results as having been predicted a priori [HARKing, short for "hypothesising after results are known"; @Kerr1998] and exploiting flexibility in data analysis to obtain statistically significant results ["p-hacking"; @Simmons2011]. 
Evidence for both categories of bias exist: 
Publication bias has been shown in peer review [@Mahoney1977; @Atkinson1982] and in longitudinal data from an NSF grant programme that found a file-drawering effect for studies with negative results [@Franco2014; @Franco2016]; and QRPs have been admitted by scientists in several survey studies [@John2012; @Fiedler2016; @Agnoli2017].

Some have argued that selecting for statistically significant results is defensible -- desirable, even -- because because it weeds out  low-quality research that would only pollute the literature [@Winter2013; but see @vanAssen2014]. 
How problematic selective publishing is in practice remains an empirical question: 
If most negative results that are currently missing from the literature are the result of immature ideas or poorly conducted studies, we should expect that a literature in which studies are selected based on their quality but not based on their results  would contain a similar proportion of positive results as the current one. 
But how many positive results would such an unbiased literature contain in reality? 
We set out to explore this question by comparing the rate of positive results in a sample from the current psychology literature to a set of studies that were published in a new format created to minimise QRPs and publication bias: Registered Reports. 

## Methods to mitigate bias
An increasingly popular proposal to reduce bias is preregistration, where authors register a time-stamped protocol of their hypotheses, planned method, and analysis plan before data collection [for a historical overview, see @Wiseman2019]. 
Preregistration is thought to mitigate QRPs by preventing HARKing (hypotheses must be stated before the results are known) and by reducing the risk of p-hacking via restricted flexibility in data analysis. 
However, preregistration does not prevent file-drawering or reviewer bias and may thus be toothless in fighting publication bias. 
A more effective safeguard against both publication bias and QRPs is promised by Registered Reports [@Chambers2013; @Nosek2014; @Chambers2015; @Jonas2016]. 
Registered Reports (RRs) are a new publication format with a restructured submission timeline: 
Before collecting data, authors submit a study protocol containing their hypotheses, planned procedures, and analysis pipeline to a journal. 
The protocol undergoes peer review, and, if successful, receives "in-principle acceptance", meaning that the journal commits to publishing the final article following data collection regardless of the statistical significance of the results. 
The authors then collect and analyse the data and write up the final report. 
The final report undergoes another round of peer review, but this time only to check if the authors adhered to the plan outlined in their protocol (and, if applicable, if the data passes pre-specified quality checks). 
RRs thus combine an antidote to QRPs (preregistration) with an antidote to publication bias, because studies are selected for publication before their results are known. 
Since its introduction at the journal Cortex in 2013, the format has rapidly gained popularity and is offered by 207 journals at the time of writing (http://cos.io/rr).

In addition to bias protection, RRs promise high-quality research: Stage-1 peer review (pre data) increases the likelihood that methodological flaws and immature or misguided ideas will be spotted and fixed (or weeded out) before a study is conducted, and authors typically have to include outcome-neutral control conditions that allow verifying data quality once results are in (studies failing these quality checks may be rejected). 
Many journals offering RRs also require that planned hypothesis tests are based on a power analysis that ensures a high probability of finding a statistically significant result if there is a true effect of the expected size (e.g., $90\%$ power for a given effect size of interest). 

Assuming a constant alpha level, the rate of positive results in a literature is influenced by three factors: the proportion of true hypotheses among all tested hypotheses, statistical power, and bias. 
The RR format combines powerful safeguards against publication bias and QRPs with standards for research quality that are at least equal to ordinary peer review, and often include statistical power requirements that likely exceed those in the standard literature [see e.g., @Maxwell2004; @Szucs2017]. 
Therefore, if the emerging RR literature in psychology contains fewer positive results than the standard literature, the cause must be either the difference in bias or a lower proportion of true hypotheses tested in RRs (or a combination of the two). 
At this time, we have good reasons to believe in a difference in bias, but little reason to believe in a difference in the proportion of true hypotheses (at least regarding original hypotheses, see below), which would make bias a more plausible explanation of a potential difference in the positive result rate (i.e., the proportion of supported hypotheses among all tested hypotheses). 
Considering the high standards for research quality in RRs, a large difference in positive results between RRs and the standard literature would also mean that publication bias is not a desirable filter for poorly conducted studies, but that we should worry about the high-quality negative results we are missing because of it.

## The current study
The goal of our study was to test if RRs in psychology show a lower positive result rate than articles published in the traditional way (henceforth referred to as "standard reports", SRs), and to estimate the size of this potential difference. 
We set out to replicate a study by @Fanelli2010 on a new sample of standard reports in psychology and compared them to all published Registered Reports in psychology. 
Fanelli searched for articles containing the phrase "test$^\ast$ the hypothes$^\ast$", drew a random sample of 150 articles per discipline, and for each of these coded if the first hypothesis mentioned in the abstract or full text had been supported or not. 
For standard reports we used the same sampling method (restricted to the psychology discipline), and for RRs we relied on a database curated by the Center for Open Science (COS). 
We chose Fanelli's method because Fanelli's -@Fanelli2010 and -@Fanelli2012 studies (both use the same coding method) have been highly influential, and his method can easily be applied to a large set of studies. 
We additionally coded if studies were replications or original work because many published RRs are replications. 
If replications are motivated by scepticism of the original results, the prior probability of hypotheses tested in these studies may be lower than in original studies, and a higher proportion of replications among RRs than standard reports could therefore introduce a confound.

In a recent commentary on benefits and challenges of open-science practices for early-career researchers, @Allen2019 conducted a similar investigation: 
They coded the proportion of null results in the 127 biomedical and psychology RRs listed in the COS database as of September 2018. 
We were not aware of their parallel efforts when we planned our study in September and October 2018. 
Allen and Mehler used a self-developed method to code the percentage of unsupported hypotheses in RRs (counting all hypotheses in each paper) and found $60.5\%$ unsupported hypotheses across all included RRs, $66\%$ for replication attempts, and $54.5\%$ for novel research. 
They compared these numbers to an estimate of $5-20\%$ null results in the standard literature. 
This $5-20\%$ estimate is based on data from @Fanelli2012, who coded only the first hypothesis of each paper [identical to @Fanelli2010], and @Cristea2018], who coded the percentage of statistically significant results in figures and tables of articles published in Nature, Science, and PNAS. 

A major advantage of our study is that it allows us to draw a more meaningful comparison between RRs and the standard literature because we apply a previously used method [@Fanelli2010; @Fanelli2012] to both groups. 
In addition, we replicate @Fanelli2010 and provide data to evaluate his method: The search term "test$^\ast$ the hypothes$^\ast$" might introduce selection effects, meaning that results obtained this way may not generalise to hypothesis-testing studies that do not use this phrase. 
Therefore we also coded the phrases used to introduce hypotheses in RRs, analysed how many of them would have been detected with Fanelli's search term, and compiled a list of alternative search terms to test the generalisability of Fanelli's results in the future. 
Finally, we share a rich dataset containing the exact quotes of hypotheses and conclusions on which we based our judgments, as well as detailed descriptions of our sampling and coding procedure (see Appendix). 
This allows others to verify (or contest) our results and provides an interesting resource for future meta-scientific research.

# Methods
After conducting a pilot to test the planned procedure we preregistered our study (https://osf.io/s8e97). 
Methods and analyses described here were preregistered unless otherwise noted. 
A detailed comparison of our preregistration and the eventual procedure is provided in the supplement. 
We report how we determined our sample size, all data exclusions, and all measures in the study.

## Sample
We used the same method as @Fanelli2010 to obtain a new sample of standard reports, but restricted year of publication to 2013-2018 to match the sample to the RR population. 
We excluded papers if they were not complete, published, non-retracted articles (e.g., meeting abstracts, study protocols without results), if they did not test a hypothesis, or if they contained insufficient information to reach a coding decision. 
An overview of the sampling process and all exclusions are shown in Figure 1. 

For standard reports we downloaded a current version of the Essential Science Indicators (ESI) database (retrieved on 4th December 2018) and used Web of Science to search for articles published between 2013 and 2018 with a Boolean search query containing the phrase "test$^\ast$ the hypothes$^\ast$" and the ISSNs of all journals listed in the ESI Psychiatry/Psychology category. 
Using the same sample size as @Fanelli2010, we randomly selected 150 papers from the 1919 search results using the sample() function in R and the seed "20190120" (not specified in our preregistration, but no other seeds were tried).
We initially excluded eight papers and replaced them (the decision to replace excluded papers was not preregistered) through the same random sampling procedure until 150 studies were found that met our criteria, but later found that two papers had erroneously been excluded, leading to a final sample size of 152 rather than 150 (see Fig. 1). 


```{r echo=FALSE, warning=FALSE, fig.cap="XXX ADD CAPTION", out.width = "\\textwidth"}

knitr::include_graphics("Sampling_exclusions_and_resampling.pdf")
```

For Registered Reports we aimed to include all published RRs in the field of Psychology that tested at least one hypothesis, regardless of whether or not they used the phrase "test$^\ast$ the hypothes$^\ast$". 
We downloaded a database of published RRs curated by the Center for Open Science[^1] (retrieved on 19th November 2018), and excluded papers published in journals that were listed in categories other than "Psychiatry/Psychology" or "Multidisciplinary" in the ESI. 
[^1]: https://www.zotero.org/groups/479248/osf/items/collectionKey/KEJP68G9 
Note that the decision to focus only on the Psychiatry/Psychology category meant excluding 13 RRs published in Cortex because the ESI counts this journal towards the separate category "Neuroscience and Behavior". 
Papers published in multidisciplinary journals and in journals not included in the ESI (e.g., Royal Society Open Science) were hand-coded by AS. 
This deviates from our preregistration insofar as we had not specified how discipline membership would be determined.
Following these exclusions, we verified the RR status of all remaining papers in our sample. 
Papers were counted as RRs if they were labelled as such by the journal itself and the journal submission guidelines made it clear that these submissions had been reviewed and received in-principle acceptance before the data collection (or analyses) of all studies in the paper had been conducted (in accordance with https://cos.io/rr). 
For papers not clearly labelled as RRs we consulted relevant editorial publications (e.g., for special issues) or contacted the respective editors directly. 
Of the 151 entries in the COS RR database, 55 were excluded because they belonged to a non-Psychology discipline, 12 because they were not certainly RRs, and 13 because they did not test hypotheses or contained insufficient information, leaving 71 RRs for the final analysis (see Fig. 1). 
Note that we excluded all eight "Registered Replication Reports" [RRRs; @Simons2014; @Simons2018] in our sample because this format explicitly focusses on effect size estimation and not hypothesis testing [@RRRwebsite].

## Measures and coding procedure
The main dependent variable was whether or not the first hypothesis was supported or not, as reported by the authors.
We tried to follow Fanelli's [-@Fanelli2010] coding procedure as closely as possible, which he describes as follows: 

> By examining the abstract and/or full- text, it was determined whether the authors of each paper had concluded to have found a positive (full or partial) or negative (null or negative) support. 
> If more than one hypothesis was being tested, only the first one to appear in the text was considered. 
> We excluded meeting abstracts and papers that either did not test a hypothesis or for which we lacked sufficient information to determine the outcome. (p. 8) 

Like @Fanelli2010, we coded hypotheses as having received "support", "partial support", or "no support", which was recoded into a binary "support" (full or partial) vs "no support" variable for the analysis. 
Coding disagreements between full and partial support were deemed minor since they would not affect the final results, thus only disagreements affecting the binary support/no support classification were treated as major and resolved through discussion.

Before preregistering our study, we conducted a pilot to assess if we could employ Fanelli's method successfully.
Originally we had planned to first reproduce his results on the same sample of Psychiatry/Psychology articles used in @Fanelli2010. 
Unfortunately the author refused to share the original data (or even a list of the coded articles) with us. 
Instead, we received an excerpt which contained data for 11 records from the original sample, but no reference information of the coded articles (personal communication, 5th October 2018). 
We were able to find these 11 articles based on the hypothesis quotes that had been coded, and used them as a pilot sample along with 10 randomly selected RRs. 
MS and AS independently coded all 21 pilot articles with only one major disagreement in each group. 
In the SR group, this disagreement was also the only case of major disagreement with Fanelli's original coding, which we deemed satisfying to proceed. 

```{r include=FALSE}
source("interrater_agreement.R")
```


Based on our experiences during the pilot, we added one coding criterion: 
If the first hypothesis mentioned in a paper was not explicitly tested but subsequently divided into sub-hypotheses that were tested, we would code the first tested hypothesis rather than the first hypothesis mentioned in the text. 
In RRs we coded the first preregistered hypothesis, thus excluding unregistered pilot studies. 
MS coded all papers in the sample, AS double-coded all papers MS had found difficult to code or could not code ($`r sum(included$coded_by_AS & included$is_RR==1)`$ RRs $= `r printnum(sum(included$coded_by_AS & included$is_RR==1)/sum(included$is_RR==1)*100)`\%$ and $`r sum(included$coded_by_AS & included$is_RR==0)`$ SRs $= `r printnum(sum(included$coded_by_AS & included$is_RR==0)/sum(included$is_RR==0)*100)` \%$). 
Only `r support.disagreement.major` disagreements were major (Cohen's kappa = `r printp(kappa.support)`) and subsequently resolved by discussion; `r support.disagreement.minor` were minor (disagreement between "support" and "partial support"). 
We preregistered that AS would additionally code a random subset of both groups, but decided against it because the number of double-coded papers seemed sufficient after double-coding only the difficult cases.

### Hypothesis introductions
Selecting papers that use the phrase "test$^\ast$ the hypothes$^\ast$" might yield different results than alternative search phrases. 
Getting a better overview of "natural" descriptions of hypotheses would be useful for future investigations of the generalisability of Fanelli's [-@Fanelli2010] results and could inspire new research questions. 
We therefore extracted the phrase used to introduce the hypothesis from the coded hypothesis quotes for all RRs and tried to identify clusters of similar expressions which may be used to create alternative search phrases. 
If MS and AS had coded different hypothesis quotes for a paper, MS's coding was used, unless disagreement resolution of the "hypothesis support" variable lead to AS's coding overruling MS's. 

### Replication status
We also wanted to code if a study was a replication of a previously published one: 
We expected a much larger proportion of RRs to be direct replications, many of which may have been motivated by scepticism of the original study. 
A lower positive result rate in RRs could then be an effect of failed replications rather than an effect of safeguards against QRPs and publication bias. 
After an initial coding attempt with ill-defined coding criteria lead to too many disagreements (described further in the Appendix), we developed the following strategy (not pre-registered): 
We searched the full texts of all papers for the string "replic$^\ast$" [cf. @Makel2012; @Kohler2019; @Mueller-Langer2019; @Pridemore2018], and for those that did contain it determined whether the first hypothesis was a close or direct replication of previously published work. 
Conceptual replications and internal replications were not counted as replications in this narrow sense: 
The reason to code replication status in the first place was the idea that direct replications of previous work may be motivated by replicators' scepticism in the hypothesis and thus have a low prior probability, whereas conceptual replications are usually considered as building on previous work rather than verifying it. 
AS coded all papers, DL double-coded `r rep.RR.coded.DL` RRs ($`r printnum(rep.RR.coded.DL/sum(included$is_RR)*100)` \%$) and `r rep.SR.coded.DL` SRs ($`r printnum(rep.RR.coded.DL/sum(included$is_RR==0)*100)` \%$). 
There were `r rep.disagreements` disagreements (Cohen's kappa = `r printp(kappa.rep)`), all were resolved by discussion. 

### Additional measures
All additional measures we collected but have not described thus far were either auxiliary variables to facilitate the coding process or earlier versions of the variables discussed above. 
All of these are documented in the Appendix and our shared dataset and codebook.


## Analysis
We planned to test our hypothesis in the following way (quoting directly from our preregistration, [https://osf.io/sy927](https://osf.io/sy927), point 9): 

> A one-sided proportion test with an alpha level of $5\%$ will be performed to test whether the positive result rate (full or partial support) of Registered Reports in psychology is statistically lower than the positive result rate of conventional reports in psychology. 
> In addition to testing if there is a statistically significant difference between RRs and conventional reports, we will test if the difference is smaller than our smallest effect size of interest using an equivalence test for proportion tests with an alpha level of $5\%$ [@Lakens2018a].
> We determined our smallest effect size of interest to be the difference between the positive result rate in psychology ($91.5\%$) and the positive result rate in general social sciences ($85.5\%$) as reported by @Fanelli2010, i.e. a difference of $91.5\% - 85.5\% = 6\%$. 
> The rationale for choosing general social sciences as a comparison is that this discipline had the lowest positive result rate amongst the "soft" sciences [@Fanelli2010]. 
> The exact percentage for general social sciences was extracted from Figure 1 in @Fanelli2010 using the software WebPlotDigitizer [@Rohatgi2018].

We would accept our hypothesis that RRs have a lower positive result rate than SRs if we found a negative difference between RRs and SRs that was significantly different from 0 *and* not statistically equivalent to a range from $-6\%$ to $+6\%$ (both at $\alpha = 5\%$). 


# Results
```{r include=FALSE}
source("quantitative_analyses.R")
```

## Confirmatory Analyses

`r n.support.SR` out of `r n.SR` SRs and `r n.support.RR` out of `r n.RR` RRs had positive results, meaning that the positive result rate was $`r printnum(prop.support.SR*100)` \%$ for SRs ($95 \%$ CI [`r printnum(min(SR.binom$conf.int)*100)`, `r printnum(max(SR.binom$conf.int)*100)`]) and $`r printnum(prop.support.RR*100)` \%$ for RRs ($95 \%$ CI [`r printnum(min(RR.binom$conf.int)*100)`, `r printnum(max(RR.binom$conf.int)*100)`]; see Fig. 1). 
The preregistered one-sided proportions test with an alpha level of $5\%$ showed that this difference of $-`r printnum((prop.support.SR-prop.support.RR)*100)` \%$ was statistically significant, $\chi^2 = `r printnum(proptestresult$statistic)`$, $p `r printp(proptestresult$p.value)`$.
Unsurprisingly, the difference was not statistically equivalent to a range between $`r -SESOI*100` \%$ and $`r SESOI*100` \%$ at $\alpha = 5\%$, $z = `r printnum(min(tostresult$TOST_z1, tostresult$TOST_z2))`$, $p `r printp(max(tostresult$TOST_p1, tostresult$TOST_p2))`$, meaning that we cannot reject differences more extreme than $`r SESOI*100`\%$. 
We thus accept our hypothesis that the positive result rate in RRs is lower than in SRs.

```{r echo=FALSE, warning=FALSE, fig.width=4, fig.height=3, fig.cap="Positive result rates for standard reports and Registered Reports. Error bars indicate 95% confidence intervals around the observed positive result rate."}
mainplot
```


## Exploratory Analyses
As described in the Method section, we only classified close or direct replications of previously published work as replications. 
This means that our non-replication category also contains some conceptual replications and "internal" replications (where original and replication are published in the same paper). 
For ease of communication we will nonetheless refer to this category as "original" studies.
As expected, direct replications were much more common among Registered Reports than standard reports: `r n.rep.RR` out of `r n.RR` RRs ($`r printnum(prop.rep.RR*100)`\%$), but only `r n.rep.SR` out of `r n.SR` SRs ($`r printnum(prop.rep.SR*100)`\%$) were classified as direct replications of previously published work. 
However, this difference cannot account for the stark overall difference between SRs and RRs described above: 
Although replication RRs in our sample indeed had a lower positive result rate than original RRs (see Table 1), the difference between original SRs and original RRs -- $`r printnum((prop.orig.support.SR-prop.orig.support.RR)*100)`\%$ -- was still significantly different from 0 ($\chi^2 = `r printnum(repproptest$statistic)`$, $p `r printp(repproptest$p.value)`$) and not statistically equivalent to a range between $`r -SESOI*100`\%$ and $`r SESOI*100`\%$ ($z = `r printnum(min(reptost$TOST_z1, reptost$TOST_z2))`$, $p `r printp(max(reptost$TOST_p1, reptost$TOST_p2))`$), both at $\alpha = 5\%$.
```{r echo = FALSE, results = 'asis'}
apa_table(
  rep.table
  , digits = c(0, 0, 0, 2, 0, 0, 0, 2, 0)
  , col.names = c("", "n", "supported", "\\%", "95\\% CI", "n", "supported", "\\%", "95\\% CI")
  , align = c("l", rep("r", 8))
  , caption = "Positive results in original studies vs replication studies"
  , note = "SRs = standard reports, RRs = Registered Reports"
  , added_stub_head = "Variables"
  , col_spanners = list(`original studies` = c(2, 5), `replication studies` = c(6, 9))
)
```

Since our SR sample represents a direct replication of @Fanelli2010 for the discipline Psychiatry & Psychology, another interesting question to ask is how our results compare to Fanelli's.
The difference between the positive result rate for SRs in our sample ($`r printnum(prop.support.SR*100)`\%$) and Fanelli's ($`r printnum(prop.support.Fanelli*100)`\%$) is $`r printnum((prop.support.SR-prop.support.Fanelli)*100)`\%$. This difference is not significantly different from 0 in a two-sided proportions test ($\chi^2 = `r printnum(Fanelliproptest$statistic)`$, $p= `r printp(Fanelliproptest$p.value)`$) but also not statistically equivalent to a range between $`r -SESOI*100`\%$ and $`r SESOI*100`\%$ ($z = `r printnum(min(Fanellitost$TOST_z1, Fanellitost$TOST_z2))`$, $p= `r printp(max(Fanellitost$TOST_p1, Fanellitost$TOST_p2))`$), both at $\alpha = 5\%$. 
In other words, we can neither reject the hypothesis that the positive result rates of the two populations are the same, nor that there is a difference of at least $\pm `r SESOI*100`\%$ between them. 
The data are inconclusive.



```{r include=FALSE}
source("hypothesis_introduction_analyses.R")
```
Finally, we analysed the language that was used to introduce or refer to hypotheses in RRs.
We were interested in whether the search phrase "test$^\ast$ the hypothes$^\ast$" used by Fanelli captured the way researchers write about hypothesis tests reasonably well. The answer is a resounding "no":
Searching the abstracts, titles, and keywords of the RR sample showed that only `r testthehypothes.RR`/`r n.RR` RRs would have been detected with this search phrase. 
To get an overview of analogous hypothesis-introduction phrases researchers used in RRs, we stripped the hypothesis quotes of RRs from all content-specific information and extracted "minimal" phrases that most distinctively indicated that a hypothesis was being described.
For example, from the hypothesis quote "(f)or Study 1, we predicted that participants reading about academic (vs. social) behaviors would show a better anagram performance" we extracted the hypothesis-introduction phrase "predicted that". 

For the majority of RRs (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==1))`), we identified one hypothesis-introduction phrase; the remaining ones used two (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==2))` RRs), three (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==3))` RRs), or four (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==4))` RR) different phrases or had no identifiable hypothesis introduction (`r length(which((hypintros$abstract.count+hypintros$fulltext.count)==0))` RR).
In this total set of `r nrow(intros.long)` hypothesis introductions, we found `r length(uniquephrases)` unique phrases, showing substantial linguistic variation (see Tables 2 and 3).
In order to condense the information, we listed all unique word stems (e.g., the word stem "hypothes$^\ast$" captures the words "hypothesis", "hypotheses", "hypothesize", "hypothesized", and so on) and analysed their frequency among all hypothesis introductions.
Excluding words that are common but too unspecific by themselves, such as "that", "to", or "whether", the five most frequent word stems were "hypothes$^\ast$" (`r wordoccurrences.df[1,2]` occurrences), "replicat$^\ast$" (`r wordoccurrences.df[2,2]`), "test$^\ast$" (`r wordoccurrences.df[3,2]`), "examine$^\ast$" (`r wordoccurrences.df[4,2]`), and "predict$^\ast$" (`r wordoccurrences.df[5,2]`). 
Clearly "test$^\ast$" and "hypothes$^\ast$" are quite popular, yet they co-occurred only `r test.and.hypothes` times and more than half of all hypothesis introductions (`r nrow(intros.long)-test.or.hypothes`/`r nrow(intros.long)`) contained neither word. Interestingly, the frequency of these two words differed between original studies and direct replications: 30 out of 43 ($`r printnum(30/43*100)`\%$) hypothesis introductions found in original RRs contained either "test$^\ast$" or "hypothes$^\ast$" or both, while only 16 out of 54 ($`r printnum(16/54*100)`\%$) hypothesis introductions in direct replication RRs did.

We noticed that direct replication RRs generally tended to use different language to describe their hypothesis. As the high frequency of the word stem "replicat$^\ast$" suggests, these studies were often not framed as new tests of a previously tested hypothesis, but as attempts to replicate a previously documented effect or to repeat a previously conducted experiment. 
Authors thus seemed to have focussed more on the goal to replicate a previous finding than to test a hypothesis.
Tables 2 and 3 list all unique hypothesis introductions and their frequency for original RRs and direct replication RRs, respectively, grouped by the five most frequent word stems ("hypothes$^\ast$", "replicat$^\ast$", "test$^\ast$", "examine$^\ast$", "predict$^\ast$"). 
Using five as a cutoff value is an arbitrary decision, but we believe that it strikes a reasonable balance between condensing the information and doing the variance of the data justice. 

```{r echo = FALSE, results = 'asis'}
apa_table(
  phrasetable.orig
  , digits = c(0, 0, 0, 0, 0)
  , col.names = c("core word(s)", "introduction phrase", "abstract", 
                  "full text", "total")
  , align = c("l", "l", rep("r", 3))
  , caption = "Hypothesis introduction phrases in original Registered Reports (testing new hypotheses)"
  , note = "This table contains 44 hypothesis introduction phrases from 30 Registered Reports: 19 papers contributed one phrase each, 9 papers contributed two each, one contributed three, and one contributed four."
  , added_stub_head = "Variables"
  , col_spanners = list(`source` = c(3, 5))
  , midrules = c(6, 16, 22, 23, 27, 32)
  , landscape = FALSE
  , font_size = "footnotesize"
  , escape = TRUE
)
```

```{r echo = FALSE, results = 'asis'}
apa_table(
  phrasetable.rep
  , digits = c(0, 0, 0, 0, 0)
  , col.names = c("core word(s)", "introduction phrase", "abstract", 
                  "full text", "total")
  , align = c("l", "l", rep("r", 3))
  , caption = "Hypothesis introduction phrases in replication Registered Reports (testing previously studied hypotheses)"
  , note = "This table contains 53 hypothesis introduction phrases from 40 Registered Reports. One additional RR had no codeable hypothesis introduction. 30 papers contributed one phrase each, 7 papers contributed two each, and three contributed three each."
  , added_stub_head = "Variables"
  , col_spanners = list(`source` = c(3, 5))
  , midrules = c(4, 5, 6, 12, 16, 33, 34, 35, 36)
  , landscape = FALSE
  , font_size = "footnotesize"
  , escape = TRUE
)
```

It is important to keep in mind that not all hypotheses could be coded from the abstract: For `r n.hypintro.fulltext.only` RRs, the hypothesis introduction phrases analysed above came only from the full text, which means that search terms extracted from them may not be useful in literature searches focussed only on titles, abstracts, and keywords. 
Therefore we additionally tested how many of the RRs would have been detected in a regular search using our five most frequent word stems. 
We searched their titles, abstracts, and keywords for "hypothes$^\ast$" OR "replicat$^\ast$" OR "test$^\ast$" OR "examine$^\ast$" OR "predict$^\ast$" and found that $`r n.hypintros.fivewords.RR`/`r n.RR`$ RRs ($`r printnum(n.hypintros.fivewords.RR/n.RR*100)` \%$) would have been detected this way. 
We do not know how well these search terms represent the population of hypothesis-testing studies in psychology, but a structured investigation of this question would be very useful for future meta-research.

# Discussion
We examined the proportion of psychology articles that find support for their first tested hypothesis and found a large difference ($`r printnum(prop.support.SR*100)` \%$ vs $`r printnum(prop.support.RR*100)` \%$) between a random sample of standard reports and the full population of Registered Reports (at the time of data analysis). 
More than half of the analysed RRs tested hypotheses based on direct replications, but the difference between standard reports and Registered Reports was still large when all direct replications were excluded from the analysis ($`r printnum(prop.orig.support.SR*100)` \%$ vs $`r printnum(prop.orig.support.RR*100)` \%$). 
The introduction of Registered Reports has clearly led to a much larger proportion of null results appearing in the published literature compared to standard reports. 

The positive result rate we found in standard reports ($`r printnum(prop.support.SR*100)` \%$) is slightly higher than the $91.5\%$ reported by @Fanelli2010, although this difference was not statistically significant. 
Our replication in a more recent sample of the psychology literature thus yielded a comparably high estimate of supported hypotheses, but we cannot rule out that the positive result rate in the population has increased since 2010 [cf. @Fanelli2012].
Furthermore, our estimate of the positive result rate for Registered Reports ($`r printnum(prop.support.RR*100)` \%$) is comparable to the $39.5\%$ reported by @Allen2019, despite some differences in method and studied population.

To explain the $`r printnum((prop.support.SR - prop.support.RR)*100)` \%$ gap between standard reports and Registered Reports, we must assume some combination of differences in bias, statistical power, or the proportion of true hypotheses researchers choose to examine. 
Figure 3 visualises the combinations of statistical power and proportion of true hypotheses that would produce the observed positive result rates if the literature were completely unbiased. 
For example, if we assume no publication bias and no QRPs, more than $90\%$ of the hypotheses authors of standard reports study must be true and study designs must have more than $90\%$ power for the true effect size. 
This is highly unlikely, meaning that the standard literature is unlikely to reflect reality. 
As we noted above, there is good reason to assume that methodological rigour and statistical power in Registered Reports are as high, or higher, than in standard reports, leaving the rate of true hypotheses and bias as remaining explanations.

```{r echo=FALSE, warning=FALSE, fig.width=6, fig.height=3, fig.cap="XXX ADD CAPTION"}
source("power_baserate_plot.R")
power.baserate.plot
```



It is a-priori plausible that Registered Reports are currently used for a population of hypotheses that are less likely to be true: 
For example, authors may use the format strategically for studies they expect to yield negative results (which would be difficult to publish otherwise). 
However, assuming over $90\%$ true hypotheses in the standard literature is neither realistic, nor would it be desirable for a science that wants to advance knowledge beyond trivial facts. 
We thus believe that this factor alone is not sufficient to explain the gap between the positive result rates in Registered Reports and standard reports. 
Rather, the numbers strongly suggest a reduction of publication bias and/or Type-1 error inflation in the Registered Reports literature.

## Limitations
We compared hypotheses tested in Registered Reports with hypotheses tested in standard reports. 
Because neither hypotheses, authors, nor editors are randomly assigned to each publication format we cannot draw firm conclusions about the causes that lead to a difference in the proportion of supported hypotheses. 
Although it seems plausible that selective reporting and QRPs are reduced in Registered Reports, we do not know by how much, nor if this reduction would be of comparable size in a randomised experiment. 
As mentioned above, it is a-priori plausible that the Registered Reports format is used selectively for particularly risky hypotheses. 
This means that the proportion of true hypotheses in Registered Reports does not necessarily generalise to the entire population of hypotheses that are tested in psychology. 
It is also important to note that our results do not warrant the conclusion that Registered Reports are effective at reducing all forms of bias. 
Authors self-select to submit Registered Reports, and the format may be particularly popular among those who try to minimise the risk of inflated error rates regardless of the report format they use. 
This would lead to less bias in the Registered Reports literature even if the format's safeguards against certain QRPs were actually ineffective. 

A second limitation of the current study [and of @Fanelli2010] is that standard reports were selected using the search term "test$^\ast$ the hypothes$^\ast$". 
As our results show, this phrase was virtually absent in the Registered Report population. 
The wide variety of ways to introduce a hypothesis we observed in Registered Reports suggests that a search for "test$^\ast$ the hypothes$^\ast$" will miss most of the hypothesis-testing studies in the psychological literature. 
Results based on this search phrase may not generalise to all published studies. 
For example, it is possible that authors are more likely to describe their research explicitly as a hypothesis test when they found positive results, but prefer more vague language for unsupported hypotheses (e.g., "we examined the role of ..."). 
If this were true, using other strategies to select standard reports might yield lower estimates for the positive result rate. 
However, this does not seem to be the case: 
Studies using different selection criteria for articles and hypotheses have found very similar rates of supported hypotheses. 
For example, the positive result rates in @Sterling1959, @Sterling1995, and the original studies included in the Reproducibility Project: Psychology [@OSC2015] were $97.28\%$, $95.56\%$, and $97\%$, respectively. 
@Motyl2017 report $89.17\%$ and $92.01\%$ significant results for "critical" hypothesis tests in papers published in 2003-2004 and 2013-2014, respectively. 
Therefore, although the search term used to find standard reports might limit the generalisability of our results, this search strategy seems to yield comparable estimates as the selection strategies used in different studies. 

A final limitation is the decision to code only the first reported hypothesis. 
The first hypothesis test may not be representative for all hypothesis tests reported in a paper, and the order of reporting may differ between standard reports and Registered Reports. 
Perhaps Registered Report authors are more likely to present their hypotheses in "chronological" order, whereas standard report authors tend to rearrange the order in which hypotheses are reported based on their outcomes, and present supported hypotheses first. 
Here again, the converging estimates from the four studies cited above (none of which use the first-hypothesis rule) make it seem unlikely that our result is an artefact of this decision. 
Regardless of which hypothesis one chooses to analyse across a set of papers -- the first, the last, or the "critical" one -- the positive result rate turns out to be higher than what can be expected based on realistic estimates of the proportion of true hypotheses researchers study and the statistical power of their tests. 

## Conclusion
Our study presents a systematic comparison of positive results in Registered Reports and the standard literature. 
The much lower positive result rate in Registered Reports compared to standard reports suggests that an unbiased literature would look very different from the published research we are used to. 
Standard publication formats seem to lead scientists to miss out on many high-quality studies with negative results, which are available in the Registered Reports literature. 
The absence of negative results is a serious threat to a cumulative science. 
Reliable protection against questionable research practices and publication bias is crucial to ensure the integrity of the literature. 
The Registered Reports format is a promising candidate for bias protection, but we are only at the beginning of understanding its effectiveness and robustness in different contexts and scientific disciplines.




# References


\setlength{\parindent}{-0.5in}
\setlength{\leftskip}{0.5in}


