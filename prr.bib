
@article{Veldkamp2018,
  title = {Preprint "{{Ensuring}} the Quality and Specificity of Preregistrations"},
  url = {https://psyarxiv.com/cdgyh},
  doi = {10.31234/OSF.IO/CDGYH},
  date = {2018},
  pages = {1--30},
  author = {Veldkamp, Coosje Lisabet Sterre and Bakker, Marjan and van Assen, Marcel A.L.M. and Crompvoets, Elise Anne Victoire and Ong, How Hwee and Soderberg, Courtney K. and Mellor, David and Nosek, Brian A. and Wicherts, Jelte},
  options = {useprefix=true}
}

@article{Lakens2018a,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  volume = {1},
  issn = {10959513},
  url = {http://journals.sagepub.com/doi/10.1177/2515245918770963},
  doi = {10.1016/j.ympev.2015.01.015},
  abstract = {The North American carnivorous pitcher plant genus Sarracenia (Sarraceniaceae) is a relatively young clade (\textbackslash{}textless3 million years ago) displaying a wide range of morphological diversity in complex trapping structures. This recently radiated group is a promising system to examine the structural evolution and diversification of carnivorous plants; however, little is known regarding evolutionary relationships within the genus. Previous attempts at resolving the phylogeny have been unsuccessful, most likely due to few parsimony-informative sites compounded by incomplete lineage sorting. Here, we applied a target enrichment approach using multiple accessions to assess the relationships of Sarracenia species. This resulted in 199 nuclear genes from 75 accessions covering the putative 8-11 species and 8 subspecies/varieties. In addition, we recovered 42. kb of plastome sequence from each accession to estimate a cpDNA-derived phylogeny. Unsurprisingly, the cpDNA had few parsimony-informative sites (0.5\%) and provided little information on species relationships. In contrast, use of the targeted nuclear loci in concatenation and coalescent frameworks elucidated many relationships within Sarracenia even with high heterogeneity among gene trees. Results were largely consistent for both concatenation and coalescent approaches. The only major disagreement was with the placement of the purpurea complex. Moreover, results suggest an Appalachian massif biogeographic origin of the genus. Overall, this study highlights the utility of target enrichment using multiple accessions to resolve relationships in recently radiated taxa.},
  number = {2},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  date = {2018},
  pages = {259--269},
  keywords = {null hypothesis,power,frequentist,equivalence testing,falsification,null-hypothesis significance test,open,tost},
  author = {Lakens, Daniël and Scheel, Anne M. and Isager, Peder M.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\B2S3ZCFD\\Lakens, Scheel, Isager - 2018 - Equivalence Testing for Psychological Research A Tutorial(2).pdf},
  eprinttype = {pmid},
  eprint = {25689607}
}

@article{Wilkinson1999,
  title = {Statistical Methods in Psychology Journals: {{Guidelines}} and Explanations},
  volume = {54},
  issn = {0003066X},
  doi = {10.1037/0003-066X.54.8.594},
  abstract = {In the light of continuing debate over the applications of significance testing in psychology journals and following the publication of J. Cohen's (1994) article, the Board of Scientific Affairs (BSA) of the American Psychological Association (APA) convened a committee called the Task Force on Statistical Interference (TFSI) whose charge was "to elucidate some of the controversial issues surrounding applications of statistics including significance testing and its alternatives; alternative underlying models and data transformation; and newer methods made possible by powerful computers" (BSA, personal communication, February 28, 1996). After extensive discussion, the BSA recommended that publishing an article in American Psychologist, as a way to initiate discussion in the field about changes in current practices of data analysis and reporting may be appropriate. This report follows that request. Following each guideline are comments, explanations, or elaborations assembled by L. Wilkinson for the task force and under its review. The report is concerned with the use of statistical methods only and is not meant as an assessment of research methods in general. The title and format of the report are adapted from an article by J. C. Bailar and F. Mosteller (1988).},
  number = {8},
  journaltitle = {American Psychologist},
  date = {1999},
  pages = {594--604},
  author = {Wilkinson, Leland},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\LNPRIGT7\\Wilkinson - 1999 - Statistical methods in psychology journals Guidelines and explanations.pdf},
  eprinttype = {pmid},
  eprint = {18793039}
}

@article{Fanelli2012,
  title = {Negative Results Are Disappearing from Most Disciplines and Countries},
  volume = {90},
  issn = {01389130},
  url = {http://link.springer.com/10.1007/s11192-011-0494-7},
  doi = {10.1007/s11192-011-0494-7},
  abstract = {Concerns that the growing competition for funding and citations might distort science are frequently discussed, but have not been verified directly. Of the hypothesized problems, perhaps the most worrying is a worsening of positive-outcome bias. A system that disfavours negative results not only distorts the scientific literature directly, but might also discourage high-risk projects and pressure scientists to fabricate and falsify their data. This study analysed over 4,600 papers published in all disciplines between 1990 and 2007, measuring the frequency of papers that, having declared to have ‘‘tested” a hypothesis, reported a positive support for it. The overall frequency of positive supports has grown by over 22\% between 1990 and 2007, with significant differences between disciplines and countries. The increase was stronger in the social and some biomedical disciplines. The United States had published, over the years, significantly fewer positive results than Asian countries (and particularly Japan) but more than European countries (and in particular the United Kingdom). Methodological artefacts cannot explain away these patterns, which support the hypotheses that research is becoming less pioneering and/or that the objectivity with which results are produced and published is decreasing.},
  number = {3},
  journaltitle = {Scientometrics},
  date = {2012-03},
  pages = {891--904},
  keywords = {Bias,Competition,Misconduct,Publication,Publish or perish,Research evaluation},
  author = {Fanelli, Daniele},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\I8CP6MMM\\Fanelli - 2012 - Negative results are disappearing from most disciplines and countries.pdf}
}

@article{Hardwicke2018,
  title = {Mapping the Universe of Registered Reports},
  issn = {2397-3374},
  url = {http://www.nature.com/articles/s41562-018-0444-y https://osf.io/preprints/bitss/fzpcy/},
  doi = {10.1038/s41562-018-0444-y},
  journaltitle = {Nature Human Behaviour},
  date = {2018-10},
  keywords = {open science,meta-research,pre-registration,journal policy,Registered Reports},
  author = {Hardwicke, Tom E. and Ioannidis, John P. A.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\4KAEC4LJ\\Unknown - Unknown - No Title.pdf}
}

@article{Chambers2015,
  title = {Registered {{Reports}}: {{Realigning}} Incentives in Scientific Publishing},
  volume = {66},
  issn = {19738102},
  doi = {10.1016/j.cortex.2015.03.022},
  abstract = {This editorial present views on realigning incentives in scientific publishing. As editors recognize this important moment for Cortex, they also take the opportunity to reiterate our view that Registered Reports should not be seen as a one-shot cure for reproducibility problems in science. The applicability of Registered Reports to different sub-fields within neuropsychology and cognitive neuroscience remains to be established; for instance, studies that rely exclusively on exploration rather than deductive hypothesis testing may not be compatible. Registered Reports present no threat to exploratory science in cases where studies include a mixture of both hypothesis testing and exploratory analyses, authors are welcome to report the outcomes of the unregistered analyses, as Sassenhagen and Bornkessel-Schlesewsky do in the current issue. Pre-registration simply allows readers to distinguish the outcomes based on a priori hypothesis testing from post hoc exploration. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  journaltitle = {Cortex},
  date = {2015},
  pages = {1--2},
  author = {Chambers, C.D. and Dienes, Z. and McIntosh, R.D. and Rotshtein, P. and Willmes, K.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\R3FSEZ2M\\Chambers et al. - 2015 - Registered Reports Realigning incentives in scientific publishing.pdf},
  eprinttype = {pmid},
  eprint = {25892410}
}

@article{Chambers2013,
  title = {Registered Reports: A New Publishing Initiative at {{Cortex}}},
  volume = {49},
  url = {http://orca.cf.ac.uk/45177/1/Chambers_Cortex_2013b_GreenOA.pdf},
  number = {3},
  date = {2013},
  pages = {609--610},
  author = {Chambers, C.D.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\9M2H8NJJ\\Chambers - 2013 - Registered reports a new publishing initiative at Cortex.pdf}
}

@article{Fanelli2010,
  title = {"{{Positive}}" Results Increase down the Hierarchy of the Sciences},
  volume = {5},
  issn = {19326203},
  url = {http://dx.plos.org/10.1371/journal.pone.0010068},
  doi = {10.1371/journal.pone.0010068},
  abstract = {The hypothesis of a Hierarchy of the Sciences with physical sciences at the top, social sciences at the bottom, and biological sciences in-between is nearly 200 years old. This order is intuitive and reflected in many features of academic life, but whether it reflects the "hardness" of scientific research–i.e., the extent to which research questions and results are determined by data and theories as opposed to non-cognitive factors–is controversial. This study analysed 2434 papers published in all disciplines and that declared to have tested a hypothesis. It was determined how many papers reported a "positive" (full or partial) or "negative" support for the tested hypothesis. If the hierarchy hypothesis is correct, then researchers in "softer" sciences should have fewer constraints to their conscious and unconscious biases, and therefore report more positive outcomes. Results confirmed the predictions at all levels considered: discipline, domain and methodology broadly defined. Controlling for observed differences between pure and applied disciplines, and between papers testing one or several hypotheses, the odds of reporting a positive result were around 5 times higher among papers in the disciplines of Psychology and Psychiatry and Economics and Business compared to Space Science, 2.3 times higher in the domain of social sciences compared to the physical sciences, and 3.4 times higher in studies applying behavioural and social methodologies on people compared to physical and chemical studies on non-biological material. In all comparisons, biological studies had intermediate values. These results suggest that the nature of hypotheses tested and the logical and methodological rigour employed to test them vary systematically across disciplines and fields, depending on the complexity of the subject matter and possibly other factors (e.g., a field's level of historical and/or intellectual development). On the other hand, these results support the scientific status of the social sciences against claims that they are completely subjective, by showing that, when they adopt a scientific approach to discovery, they differ from the natural sciences only by a matter of degree.},
  number = {4},
  journaltitle = {PLoS ONE},
  date = {2010-04},
  pages = {e10068},
  author = {Fanelli, Daniele},
  editor = {Scalas, Enrico},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\MRWQV8RG\\Fanelli - 2010 - “Positive” Results Increase Down the Hierarchy of the Sciences.pdf},
  eprinttype = {pmid},
  eprint = {20383332}
}

@book{Rohatgi2018,
  location = {{Austin, Texas, USA}},
  title = {{{WebPlotDigitizer}} - {{Web Based Plot Digitizer}}},
  url = {https://automeris.io/WebPlotDigitizer},
  date = {2018},
  author = {Rohatgi, A.}
}

@book{CenterforOpenSciencea,
  title = {{{OSF}} - {{Registered Reports}}},
  url = {https://www.zotero.org/groups/479248/osf/items/collectionKey/KEJP68G9?},
  urldate = {2018-10-26},
  author = {{Center for Open Science}}
}

@article{Ferguson2012,
  title = {A {{Vast Graveyard}} of {{Undead Theories}}: {{Publication Bias}} and {{Psychological Science}}'s {{Aversion}} to the {{Null}}},
  volume = {7},
  issn = {17456916},
  doi = {10.1177/1745691612459059},
  abstract = {Publication bias remains a controversial issue in psychological science. The tendency of psychological science to avoid publishing null results produces a situation that limits the replicability assumption of science, as replication cannot be meaningful without the potential acknowledgment of failed replications. We argue that the field often constructs arguments to block the publication and interpretation of null results and that null results may be further extinguished through questionable researcher practices. Given that science is dependent on the process of falsification, we argue that these problems reduce psychological science's capability to have a proper mechanism for theory falsification, thus resulting in the promulgation of numerous “undead” theories that are ideologically popular but have little basis in fact.},
  number = {6},
  journaltitle = {Perspectives on Psychological Science},
  date = {2012},
  pages = {555--561},
  keywords = {publication bias,null hypothesis significance testing,falsification,fail-safe number,meta-analyses},
  author = {Ferguson, Christopher J. and Heene, Moritz},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\G5RZ335H\\Ferguson, Heene - 2012 - A Vast Graveyard of Undead Theories Publication Bias and Psychological Science's Aversion to the Null(2).pdf},
  eprinttype = {pmid},
  eprint = {26168112}
}

@article{Rosenthal1979,
  title = {The File Drawer Problem and Tolerance for Null Results},
  volume = {86},
  issn = {00332909},
  url = {http://content.apa.org/journals/bul/86/3/638},
  doi = {10.1037/0033-2909.86.3.638},
  abstract = {For any gien research area, one cannot tell how many studies have been conducted but never reported. The extreme view of the "file drawer problem" is that journals are filled with the 5\% of the studies that show Type 1 errors, while the file drawers are filled with the 95\% of the studies that show non-significant resluts. Quantitative procedures for computing the tolerance for filed and future results are reported and illustrated, and the implications are discussed.},
  number = {3},
  journaltitle = {Psychological Bulletin},
  date = {1979},
  pages = {638--641},
  keywords = {tolerance for null results bias in publication of},
  author = {Rosenthal, Robert},
  eprinttype = {pmid},
  eprint = {53}
}

@article{Kerr1998,
  title = {{{HARKing}}: {{Hypothesizing}} after the Results Are Known},
  volume = {2},
  issn = {10888683},
  url = {http://journals.sagepub.com/doi/10.1207/s15327957pspr0203_4},
  doi = {10.1207/s15327957pspr0203_4},
  abstract = {This article considers a practice in scientific communication termed HARKing (Hypothesizing After the Results are Known). HARKing is defined as presenting a post hoc hypothesis (i.e., one based on or informed by one's results) in one's research report as i f it were, in fact, an a priori hypotheses. Several forms of HARKing are identified and survey data are presented that suggests that at least some forms of HARKing are widely practiced and widely seen as inappropriate. I identify several reasons why scientists might HARK. Then I discuss several reasons why scientists ought not to HARK. It is conceded that the question of whether HARKing ' s costs exceed its benefits is a complex one that ought to be addressed through research, open discussion, and debate. To help stimulate such discussion (and for those such as myself who suspect that HARKing's costs do exceed its benefits), I conclude the article with some suggestions for deterring HARKing.},
  number = {3},
  journaltitle = {Personality and Social Psychology Review},
  date = {1998-08},
  pages = {196--217},
  author = {Kerr, Norbert L.},
  eprinttype = {pmid},
  eprint = {15647155}
}

@article{Sterling1959,
  title = {Publication {{Decisions}} and Their {{Possible Effects}} on {{Inferences Drawn}} from {{Tests}} of {{Significance}}—or {{Vice Versa}}},
  volume = {54},
  issn = {1537274X},
  url = {http://www.tandfonline.com/doi/abs/10.1080/01621459.1959.10501497},
  doi = {10.1080/01621459.1959.10501497},
  abstract = {There is some evidence that in fields where statistical tests of significance are commonly used, research which yields nonsignificant results is not published. Such research being unknown to other investigators may be repeated independently until eventually by chance a significant result occurs-an "error of the first kind"-and is published. Significant results published in these fields are seldom verified by independent replication. The possibility thus arises that the literature of such a field consists in substantial part of false conclusions resulting from errors of the first kind in statistical tests of significance.},
  number = {285},
  journaltitle = {Journal of the American Statistical Association},
  date = {1959-03},
  pages = {30--34},
  author = {Sterling, Theodore D.},
  eprinttype = {pmid},
  eprint = {25246403}
}

@book{AssociationforPsychologicalScience,
  title = {Registered {{Replication Reports}}},
  url = {https://www.psychologicalscience.org/publications/replication},
  urldate = {2019-01-27},
  author = {{Association for Psychological Science}}
}

@article{Simmons2011,
  title = {False-Positive Psychology: {{Undisclosed}} Flexibility in Data Collection and Analysis Allows Presenting Anything as Significant},
  volume = {22},
  issn = {14679280},
  url = {http://journals.sagepub.com/doi/10.1177/0956797611417632},
  doi = {10.1177/0956797611417632},
  abstract = {In this article, we accomplish two things. First, we show that despite empirical psychologists' nominal endorsement of a low rate of false-positive findings (≤ .05), flexibility in data collection, analysis, and reporting dramatically increases actual false-positive rates. In many cases, a researcher is more likely to falsely find evidence that an effect exists than to correctly find evidence that it does not. We present computer simulations and a pair of actual experiments that demonstrate how unacceptably easy it is to accumulate (and report) statistically significant evidence for a false hypothesis. Second, we suggest a simple, low-cost, and straightforwardly effective disclosure-based solution to this problem. The solution involves six concrete requirements for authors and four guidelines for reviewers, all of which impose a minimal burden on the publication process.},
  number = {11},
  journaltitle = {Psychological Science},
  date = {2011-11},
  pages = {1359--1366},
  keywords = {methodology,disclosure,motivated reasoning,publication},
  author = {Simmons, Joseph P. and Nelson, Leif D. and Simonsohn, Uri},
  eprinttype = {pmid},
  eprint = {22006061}
}

@article{John2012,
  title = {Measuring the {{Prevalence}} of {{Questionable Research Practices With Incentives}} for {{Truth Telling}}},
  volume = {23},
  issn = {14679280},
  url = {http://journals.sagepub.com/doi/10.1177/0956797611430953},
  doi = {10.1177/0956797611430953},
  abstract = {Cases of clear scientific misconduct have received significant media attention recently, but less flagrantly questionable research practices may be more prevalent and, ultimately, more damaging to the academic enterprise. Using an anonymous elicitation format supplemented by incentives for honest reporting, we surveyed over 2,000 psychologists about their involvement in questionable research practices. The impact of truth-telling incentives on self-admissions of questionable research practices was positive, and this impact was greater for practices that respondents judged to be less defensible. Combining three different estimation methods, we found that the percentage of respondents who have engaged in questionable practices was surprisingly high. This finding suggests that some questionable practices may constitute the prevailing research norm.},
  number = {5},
  journaltitle = {Psychological Science},
  date = {2012-05},
  pages = {524--532},
  keywords = {judgment,methodology,disclosure,professional standards},
  author = {John, Leslie K. and Loewenstein, George and Prelec, Drazen},
  eprinttype = {pmid},
  eprint = {22508865}
}

@article{Kohler2019,
  langid = {english},
  title = {Play {{It Again}}, {{Sam}}! {{An Analysis}} of {{Constructive Replication}} in the {{Organizational Sciences}}},
  issn = {0149-2063, 1557-1211},
  url = {http://journals.sagepub.com/doi/10.1177/0149206319843985},
  doi = {10.1177/0149206319843985},
  journaltitle = {Journal of Management},
  urldate = {2019-12-19},
  date = {2019-04-24},
  pages = {014920631984398},
  author = {Köhler, Tine and Cortina, Jose M.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\XPLXZP4E\\Köhler and Cortina - 2019 - Play It Again, Sam! An Analysis of Constructive Re.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\H8J8TRDC\\10.1177@0149206319843985.html}
}

@article{Mueller-Langer2019,
  langid = {english},
  title = {Replication Studies in Economics—{{How}} Many and Which Papers Are Chosen for Replication, and Why?},
  volume = {48},
  issn = {0048-7333},
  url = {http://www.sciencedirect.com/science/article/pii/S0048733318301847},
  doi = {10.1016/j.respol.2018.07.019},
  abstract = {We investigate how often replication studies are published in empirical economics and what types of journal articles are replicated. We find that between 1974 and 2014 0.1\% of publications in the top 50 economics journals were replication studies. We consider the results of published formal replication studies (whether they are negating or reinforcing) and their extent: Narrow replication studies are typically devoted to mere replication of prior work, while scientific replication studies provide a broader analysis. We find evidence that higher-impact articles and articles by authors from leading institutions are more likely to be replicated, whereas the replication probability is lower for articles that appeared in top 5 economics journals. Our analysis also suggests that mandatory data disclosure policies may have a positive effect on the incidence of replication.},
  number = {1},
  journaltitle = {Research Policy},
  shortjournal = {Research Policy},
  urldate = {2019-12-19},
  date = {2019-02-01},
  pages = {62-83},
  keywords = {Replication,Science policy,Economic methodology,Economics of science},
  author = {Mueller-Langer, Frank and Fecher, Benedikt and Harhoff, Dietmar and Wagner, Gert G.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\WEVGHMYH\\Mueller-Langer et al. - 2019 - Replication studies in economics—How many and whic.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\5KB6624U\\S0048733318301847.html}
}

@article{Pridemore2018,
  title = {Replication in {{Criminology}} and the {{Social Sciences}}},
  volume = {1},
  issn = {2572-4568},
  url = {https://www.annualreviews.org/doi/10.1146/annurev-criminol-032317-091849},
  doi = {10.1146/annurev-criminol-032317-091849},
  abstract = {Replication is a hallmark of science. In recent years, some medical sciences and behavioral sciences struggled with what came to be known as replication crises. As a field, criminology has yet to address formally the threats to our evidence base that might be posed by large-scale and systematic replication attempts, although it is likely we would face challenges similar to those experienced by other disciplines. In this review, we outline the basics of replication, summarize reproducibility problems found in other fields, undertake an original analysis of the amount and nature of replication studies appearing in criminology journals, and consider how criminology can begin to assess more formally the robustness of our knowledge through encouraging a culture of replication.},
  number = {1},
  journaltitle = {Annual Review of Criminology},
  shortjournal = {Annu. Rev. Criminol.},
  urldate = {2019-12-19},
  date = {2018-01-13},
  pages = {19-38},
  keywords = {replication,meta-science,criminology},
  author = {Pridemore, William Alex and Makel, Matthew C. and Plucker, Jonathan A.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\EAQDMR4J\\Pridemore et al. - 2018 - Replication in Criminology and the Social Sciences.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\HUNKK742\\annurev-criminol-032317-091849.html}
}

@article{Szucs2017,
  langid = {english},
  title = {Empirical Assessment of Published Effect Sizes and Power in the Recent Cognitive Neuroscience and Psychology Literature},
  volume = {15},
  issn = {1545-7885},
  url = {http://dx.plos.org/10.1371/journal.pbio.2000797},
  doi = {10.1371/journal.pbio.2000797},
  number = {3},
  journaltitle = {PLOS Biology},
  urldate = {2018-06-14},
  date = {2017},
  pages = {e2000797},
  author = {Szucs, Denes and Ioannidis, John P. A.},
  editor = {Wagenmakers, Eric-Jan},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\L266E2WY\\Szucs and Ioannidis - 2017 - Empirical assessment of published effect sizes and.pdf}
}

@article{Maxwell2004,
  langid = {english},
  title = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}: {{Causes}}, {{Consequences}}, and {{Remedies}}.},
  volume = {9},
  issn = {1939-1463, 1082-989X},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/1082-989X.9.2.147},
  doi = {10.1037/1082-989X.9.2.147},
  shorttitle = {The {{Persistence}} of {{Underpowered Studies}} in {{Psychological Research}}},
  number = {2},
  journaltitle = {Psychological Methods},
  urldate = {2019-01-31},
  date = {2004},
  pages = {147-163},
  keywords = {stats,power,replication crisis,meta-science},
  author = {Maxwell, Scott E.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\F4GWKHXT\\Maxwell - 2004 - The Persistence of Underpowered Studies in Psychol.pdf}
}

@article{Winter2013,
  langid = {english},
  title = {Why {{Selective Publication}} of {{Statistically Significant Results Can Be Effective}}},
  volume = {8},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0066463},
  doi = {10.1371/journal.pone.0066463},
  abstract = {Concerns exist within the medical and psychological sciences that many published research findings are not replicable. Guidelines accordingly recommend that the file drawer effect should be eliminated and that statistical significance should not be a criterion in the decision to submit and publish scientific results. By means of a simulation study, we show that selectively publishing effects that differ significantly from the cumulative meta-analytic effect evokes the Proteus phenomenon of poorly replicable and alternating findings. However, the simulation also shows that the selective publication approach yields a scientific record that is content rich as compared to publishing everything, in the sense that fewer publications are needed for obtaining an accurate meta-analytic estimation of the true effect. We conclude that, under the assumption of self-correcting science, the file drawer effect can be beneficial for the scientific collective.},
  number = {6},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-12-19},
  date = {2013-06-20},
  pages = {e66463},
  keywords = {Meta-analysis,Research validity,stats,power,Heart,publication bias,Medicine and health sciences,Replication studies,Mental health and psychiatry,Scientific publishing,Research design,meta-science},
  author = {de Winter, Joost and Happee, Riender},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\WIPAPHSF\\Winter and Happee - 2013 - Why Selective Publication of Statistically Signifi.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\RFE5E37E\\article.html}
}

@article{vanAssen2014,
  langid = {english},
  title = {Why {{Publishing Everything Is More Effective}} than {{Selective Publishing}} of {{Statistically Significant Results}}},
  volume = {9},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0084896},
  doi = {10.1371/journal.pone.0084896},
  abstract = {Background De Winter and Happee [1] examined whether science based on selective publishing of significant results may be effective in accurate estimation of population effects, and whether this is even more effective than a science in which all results are published (i.e., a science without publication bias). Based on their simulation study they concluded that “selective publishing yields a more accurate meta-analytic estimation of the true effect than publishing everything, (and that) publishing nonreplicable results while placing null results in the file drawer can be beneficial for the scientific collective” (p.4). Methods and Findings Using their scenario with a small to medium population effect size, we show that publishing everything is more effective for the scientific collective than selective publishing of significant results. Additionally, we examined a scenario with a null effect, which provides a more dramatic illustration of the superiority of publishing everything over selective publishing. Conclusion Publishing everything is more effective than only reporting significant outcomes.},
  number = {1},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-12-19},
  date = {2014-01-17},
  pages = {e84896},
  keywords = {Meta-analysis,Statistical methods,Social sciences,publication bias,Scientists,Simulation and modeling,Publication ethics,Normal distribution,Scientific publishing,meta-science},
  author = {van Assen, Marcel A. L. M. and van Aert, Robbie C. M. and Nuijten, Michèle B. and Wicherts, Jelte M.},
  options = {useprefix=true},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\P2ZBPUXC\\Assen et al. - 2014 - Why Publishing Everything Is More Effective than S.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\IVJF37JZ\\article.html}
}

@article{Makel2012,
  langid = {english},
  title = {Replications in {{Psychology Research}}: {{How Often Do They Really Occur}}?},
  url = {https://journals.sagepub.com/doi/full/10.1177/1745691612460688},
  doi = {10.1177/1745691612460688},
  shorttitle = {Replications in {{Psychology Research}}},
  abstract = {Recent controversies in psychology have spurred conversations about the nature and quality of psychological research. One topic receiving substantial attention ...},
  journaltitle = {Perspectives on Psychological Science},
  urldate = {2020-01-03},
  date = {2012-11-07},
  keywords = {replication,meta-science},
  author = {Makel, Matthew C. and Plucker, Jonathan A. and Hegarty, Boyd},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\7VM92BTE\\makel2012.html}
}

@article{Lakens2019b,
  title = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}: {{A Conceptual Analysis}}},
  url = {https://psyarxiv.com/jbh4w/},
  doi = {10.31234/osf.io/jbh4w},
  shorttitle = {The {{Value}} of {{Preregistration}} for {{Psychological Science}}},
  abstract = {For over two centuries researchers have been criticized for using research practices that makes it easier to present data in line with what they wish to be true. With the rise of the internet it has become easier to preregister the theoretical and empirical basis for predictions, the experimental design, the materials, and the analysis code. Whether the practice of preregistration is valuable depends on your philosophy of science. Here, I provide a conceptual analysis of the value of preregistration for psychological science from an error statistical philosophy (Mayo, 2018). Preregistration has the goal to allow others to transparently evaluate the capacity of a test to falsify a prediction, or the severity of a test. Researchers who aim to test predictions with severity should find value in the practice of preregistration. I differentiate the goal of preregistration from positive externalities, discuss how preregistration itself does not make a study better or worse compared to a non-preregistered study, and highlight the importance of evaluating the usefulness of a tool such as preregistration based on an explicit consideration of your philosophy of science.},
  urldate = {2019-12-30},
  date = {2019-11-18T09:17:34.500Z},
  keywords = {preregistration,meta-science},
  author = {Lakens, Daniel},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\N6X3JG8Z\\Lakens_2019_The Value of Preregistration for Psychological Science.pdf}
}

@article{Motyl2017,
  langid = {english},
  title = {The State of Social and Personality Science: {{Rotten}} to the Core, Not so Bad, Getting Better, or Getting Worse?},
  volume = {113},
  issn = {1939-1315, 0022-3514},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/pspa0000084},
  doi = {10.1037/pspa0000084},
  shorttitle = {The State of Social and Personality Science},
  number = {1},
  journaltitle = {Journal of Personality and Social Psychology},
  urldate = {2018-06-14},
  date = {2017},
  pages = {34-58},
  author = {Motyl, Matt and Demos, Alexander P. and Carsel, Timothy S. and Hanson, Brittany E. and Melton, Zachary J. and Mueller, Allison B. and Prims, J. P. and Sun, Jiaqing and Washburn, Anthony N. and Wong, Kendal M. and Yantis, Caitlyn and Skitka, Linda J.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\USTJI4IV\\Motyl et al_2017_The state of social and personality science.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\NET52V32\\motyl2017.html}
}

@article{OSC2015,
  langid = {english},
  title = {Estimating the Reproducibility of Psychological Science},
  volume = {349},
  issn = {0036-8075, 1095-9203},
  url = {https://science.sciencemag.org/content/349/6251/aac4716},
  doi = {10.1126/science.aac4716},
  abstract = {Empirically analyzing empirical evidence
One of the central goals in any scientific endeavor is to understand causality. Experiments that seek to demonstrate a cause/effect relation most often manipulate the postulated causal factor. Aarts et al. describe the replication of 100 experiments reported in papers published in 2008 in three high-ranking psychology journals. Assessing whether the replication and the original experiment yielded the same result according to several criteria, they find that about one-third to one-half of the original findings were also observed in the replication study.
Science, this issue 10.1126/science.aac4716
Structured Abstract
INTRODUCTIONReproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. Scientific claims should not gain credence because of the status or authority of their originator but by the replicability of their supporting evidence. Even research of exemplary quality may have irreproducible empirical findings because of random or systematic error.
RATIONALEThere is concern about the rate and predictors of reproducibility, but limited evidence. Potentially problematic practices include selective reporting, selective analysis, and insufficient specification of the conditions necessary or sufficient to obtain the results. Direct replication is the attempt to recreate the conditions believed sufficient for obtaining a previously observed finding and is the means of establishing reproducibility of a finding with new data. We conducted a large-scale, collaborative effort to obtain an initial estimate of the reproducibility of psychological science.
RESULTSWe conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. There is no single standard for evaluating replication success. Here, we evaluated reproducibility using significance and P values, effect sizes, subjective assessments of replication teams, and meta-analysis of effect sizes. The mean effect size (r) of the replication effects (Mr = 0.197, SD = 0.257) was half the magnitude of the mean effect size of the original effects (Mr = 0.403, SD = 0.188), representing a substantial decline. Ninety-seven percent of original studies had significant results (P {$<$} .05). Thirty-six percent of replications had significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
CONCLUSIONNo single indicator sufficiently describes replication success, and the five indicators examined here are not the only ways to evaluate reproducibility. Nonetheless, collectively these results offer a clear conclusion: A large portion of replications produced weaker evidence for the original findings despite using materials provided by the original authors, review in advance for methodological fidelity, and high statistical power to detect the original effect sizes. Moreover, correlational evidence is consistent with the conclusion that variation in the strength of initial evidence (such as original P value) was more predictive of replication success than variation in the characteristics of the teams conducting the research (such as experience and expertise). The latter factors certainly can influence replication success, but they did not appear to do so here.Reproducibility is not well understood because the incentives for individual scientists prioritize novelty over replication. Innovation is the engine of discovery and is vital for a productive, effective scientific enterprise. However, innovative ideas become old news fast. Journal reviewers and editors may dismiss a new test of a published idea as unoriginal. The claim that “we already know this” belies the uncertainty of scientific evidence. Innovation points out paths that are possible; replication points out paths that are likely; progress relies on both. Replication can increase certainty when findings are reproduced and promote innovation when they are not. This project provides accumulating evidence for many findings in psychological research and suggests that there is still more work to do to verify whether we know what we think we know. {$<$}img class="fragment-image" aria-describedby="F1-caption" src="https://science.sciencemag.org/content/sci/349/6251/aac4716/F1.medium.gif"/{$>$} Download high-res image Open in new tab Download Powerpoint Original study effect size versus replication effect size (correlation coefficients).Diagonal line represents replication effect size equal to original effect size. Dotted line represents replication effect size of 0. Points below the dotted line were effects in the opposite direction of the original. Density plots are separated by significant (blue) and nonsignificant (red) effects.
Reproducibility is a defining feature of science, but the extent to which it characterizes current research is unknown. We conducted replications of 100 experimental and correlational studies published in three psychology journals using high-powered designs and original materials when available. Replication effects were half the magnitude of original effects, representing a substantial decline. Ninety-seven percent of original studies had statistically significant results. Thirty-six percent of replications had statistically significant results; 47\% of original effect sizes were in the 95\% confidence interval of the replication effect size; 39\% of effects were subjectively rated to have replicated the original result; and if no bias in original results is assumed, combining original and replication results left 68\% with statistically significant effects. Correlational tests suggest that replication success was better predicted by the strength of original evidence than by characteristics of the original and replication teams.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.
A large-scale assessment suggests that experimental reproducibility in psychology leaves a lot to be desired.},
  number = {6251},
  journaltitle = {Science},
  urldate = {2019-12-19},
  date = {2015-08-28},
  pages = {aac4716},
  author = {Open Science Collaboration},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\I3Q3B96D\\estimating-the-reproducibility-of-psychological-science-2015.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\QHHSM9J6\\Open Science Collaboration_2015_Estimating the reproducibility of psychological science.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\L8VBSN5A\\aac4716.html},
  eprinttype = {pmid},
  eprint = {26315443}
}

@online{RRRwebsite,
  langid = {american},
  title = {Registered {{Replication Reports}}},
  url = {https://www.psychologicalscience.org/publications/replication},
  abstract = {Quick Links

 	Mission Statement
 	Article Type Description
 	Instructions for Authors
 	Instructions for Reviewers
 	Ongoing Replication Projects

Mission Statement
Replicability is a cornerstone of science. Yet replication studies rarely appear in psychology journals. The new Registered Replication Reports …},
  journaltitle = {Association for Psychological Science - APS},
  urldate = {2019-12-28}
}

@article{Simons2018,
  langid = {english},
  title = {Introducing {{Advances}} in {{Methods}} and {{Practices}} in {{Psychological Science}}},
  volume = {1},
  issn = {2515-2459},
  url = {https://doi.org/10.1177/2515245918757424},
  doi = {10.1177/2515245918757424},
  number = {1},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  shortjournal = {Advances in Methods and Practices in Psychological Science},
  urldate = {2019-09-27},
  date = {2018-03-01},
  pages = {3-6},
  author = {Simons, Daniel J.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\2XAQRNKU\\Simons - 2018 - Introducing Advances in Methods and Practices in P.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\ZSZ25H93\\Simons_2018_Introducing Advances in Methods and Practices in Psychological Science.pdf}
}

@article{Simons2014,
  langid = {english},
  title = {An {{Introduction}} to {{Registered Replication Reports}} at {{{\emph{Perspectives}}}}{\emph{ on }}{{{\emph{Psychological Science}}}}},
  volume = {9},
  issn = {1745-6916, 1745-6924},
  url = {http://journals.sagepub.com/doi/10.1177/1745691614543974},
  doi = {10.1177/1745691614543974},
  number = {5},
  journaltitle = {Perspectives on Psychological Science},
  urldate = {2018-06-08},
  date = {2014},
  pages = {552-555},
  author = {Simons, Daniel J. and Holcombe, Alex O. and Spellman, Barbara A.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\QA6JJYF4\\simons2014.html}
}

@article{Cristea2018,
  langid = {english},
  title = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant: {{A}} Survey of Top Science Journals},
  volume = {13},
  issn = {1932-6203},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0197440},
  doi = {10.1371/journal.pone.0197440},
  shorttitle = {P Values in Display Items Are Ubiquitous and Almost Invariably Significant},
  abstract = {P values represent a widely used, but pervasively misunderstood and fiercely contested method of scientific inference. Display items, such as figures and tables, often containing the main results, are an important source of P values. We conducted a survey comparing the overall use of P values and the occurrence of significant P values in display items of a sample of articles in the three top multidisciplinary journals (Nature, Science, PNAS) in 2017 and, respectively, in 1997. We also examined the reporting of multiplicity corrections and its potential influence on the proportion of statistically significant P values. Our findings demonstrated substantial and growing reliance on P values in display items, with increases of 2.5 to 14.5 times in 2017 compared to 1997. The overwhelming majority of P values (94\%, 95\% confidence interval [CI] 92\% to 96\%) were statistically significant. Methods to adjust for multiplicity were almost non-existent in 1997, but reported in many articles relying on P values in 2017 (Nature 68\%, Science 48\%, PNAS 38\%). In their absence, almost all reported P values were statistically significant (98\%, 95\% CI 96\% to 99\%). Conversely, when any multiplicity corrections were described, 88\% (95\% CI 82\% to 93\%) of reported P values were statistically significant. Use of Bayesian methods was scant (2.5\%) and rarely (0.7\%) articles relied exclusively on Bayesian statistics. Overall, wider appreciation of the need for multiplicity corrections is a welcome evolution, but the rapid growth of reliance on P values and implausibly high rates of reported statistical significance are worrisome.},
  number = {5},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  urldate = {2019-10-01},
  date = {2018-05-15},
  pages = {e0197440},
  keywords = {Meta-analysis,Statistical data,Analysis of variance,Bayesian method,Scientific publishing,Software tools,Computer software,Bayesian statistics},
  author = {Cristea, Ioana Alina and Ioannidis, John P. A.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\29VUKUIT\\Cristea_Ioannidis_2018_P values in display items are ubiquitous and almost invariably significant.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\XK99CE6E\\Cristea and Ioannidis - 2018 - P values in display items are ubiquitous and almos.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\3IM6AMFL\\article.html}
}

@article{Allen2019,
  langid = {english},
  title = {Open Science Challenges, Benefits and Tips in Early Career and Beyond},
  volume = {17},
  issn = {1545-7885},
  url = {http://dx.plos.org/10.1371/journal.pbio.3000246},
  doi = {10.1371/journal.pbio.3000246},
  number = {5},
  journaltitle = {PLOS Biology},
  urldate = {2019-08-28},
  date = {2019-05-01},
  pages = {e3000246},
  author = {Allen, Christopher and Mehler, David M. A.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\AEDWM5CB\\Allen_Mehler_2019_Open science challenges, benefits and tips in early career and beyond.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\6VPKWFWI\\allen2019.html}
}

@article{Jonas2016,
  langid = {english},
  title = {How Can Preregistration Contribute to Research in Our Field?},
  volume = {1},
  issn = {2374-3603, 2374-3611},
  url = {https://www.tandfonline.com/doi/full/10.1080/23743603.2015.1070611},
  doi = {10.1080/23743603.2015.1070611},
  number = {1-3},
  journaltitle = {Comprehensive Results in Social Psychology},
  urldate = {2018-05-23},
  date = {2016},
  pages = {1-7},
  author = {Jonas, Kai J. and Cesario, Joseph},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\V744SS8Q\\Jonas and Cesario - 2016 - How can preregistration contribute to research in .pdf}
}

@article{Nosek2014,
  langid = {english},
  title = {Registered {{Reports}}: {{A Method}} to {{Increase}} the {{Credibility}} of {{Published Results}}},
  volume = {45},
  issn = {1864-9335, 2151-2590},
  url = {http://econtent.hogrefe.com/doi/abs/10.1027/1864-9335/a000192},
  doi = {10.1027/1864-9335/a000192},
  shorttitle = {Registered {{Reports}}},
  number = {3},
  journaltitle = {Social Psychology},
  urldate = {2018-05-23},
  date = {2014},
  pages = {137-141},
  author = {Nosek, Brian A. and Lakens, Daniël},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\RUZMCATF\\Nosek_Lakens_2014_Registered Reports.pdf}
}

@article{Wiseman2019,
  langid = {english},
  title = {Registered Reports: An Early Example and Analysis},
  volume = {7},
  issn = {2167-8359},
  url = {https://peerj.com/articles/6232},
  doi = {10.7717/peerj.6232},
  shorttitle = {Registered Reports},
  abstract = {The recent ‘replication crisis’ in psychology has focused attention on ways of increasing methodological rigor within the behavioral sciences. Part of this work has involved promoting ‘Registered Reports’, wherein journals peer review papers prior to data collection and publication. Although this approach is usually seen as a relatively recent development, we note that a prototype of this publishing model was initiated in the mid-1970s by parapsychologist Martin Johnson in the European Journal of Parapsychology (EJP). A retrospective and observational comparison of Registered and non-Registered Reports published in the EJP during a seventeen-year period provides circumstantial evidence to suggest that the approach helped to reduce questionable research practices. This paper aims both to bring Johnson’s pioneering work to a wider audience, and to investigate the positive role that Registered Reports may play in helping to promote higher methodological and statistical standards.},
  journaltitle = {PeerJ},
  shortjournal = {PeerJ},
  urldate = {2019-12-28},
  date = {2019-01-16},
  pages = {e6232},
  keywords = {meta-science,RRs,history of science,parapsychology},
  author = {Wiseman, Richard and Watt, Caroline and Kornbrot, Diana},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\EJ73H6YG\\6232.html}
}

@article{Fiedler2016,
  langid = {english},
  title = {Questionable {{Research Practices Revisited}}},
  volume = {7},
  issn = {1948-5506, 1948-5514},
  url = {http://journals.sagepub.com/doi/10.1177/1948550615612150},
  doi = {10.1177/1948550615612150},
  abstract = {The current discussion of questionable research practices (QRPs) is meant to improve the quality of science. It is, however, important to conduct QRP studies with the same scrutiny as all research. We note problems with overestimates of QRP prevalence and the survey methods used in the frequently cited study by John, Loewenstein, and Prelec. In a survey of German psychologists, we decomposed QRP prevalence into its two multiplicative components, proportion of scientists who ever committed a behavior and, if so, how frequently they repeated this behavior across all their research. The resulting prevalence estimates are lower by order of magnitudes. We conclude that inflated prevalence estimates, due to problematic interpretation of survey data, can create a descriptive norm (QRP is normal) that can counteract the injunctive norm to minimize QRPs and unwantedly damage the image of behavioral sciences, which are essential to dealing with many societal problems.},
  number = {1},
  journaltitle = {Social Psychological and Personality Science},
  urldate = {2019-09-23},
  date = {2016-01},
  pages = {45-52},
  author = {Fiedler, Klaus and Schwarz, Norbert},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\NXEEHHRI\\Fiedler_Schwarz_2016_Questionable Research Practices Revisited.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\F2VB6CVY\\fiedler2015.html}
}

@article{Agnoli2017,
  title = {Questionable Research Practices among Italian Research Psychologists},
  volume = {12},
  url = {https://doi.org/10.1371/journal.pone.0172792},
  doi = {10.1371/journal.pone.0172792},
  abstract = {A survey in the United States revealed that an alarmingly large percentage of university psychologists admitted having used questionable research practices that can contaminate the research literature with false positive and biased findings. We conducted a replication of this study among Italian research psychologists to investigate whether these findings generalize to other countries. All the original materials were translated into Italian, and members of the Italian Association of Psychology were invited to participate via an online survey. The percentages of Italian psychologists who admitted to having used ten questionable research practices were similar to the results obtained in the United States although there were small but significant differences in self-admission rates for some QRPs. Nearly all researchers (88\%) admitted using at least one of the practices, and researchers generally considered a practice possibly defensible if they admitted using it, but Italian researchers were much less likely than US researchers to consider a practice defensible. Participants' estimates of the percentage of researchers who have used these practices were greater than the self-admission rates, and participants estimated that researchers would be unlikely to admit it. In written responses, participants argued that some of these practices are not questionable and they have used some practices because reviewers and journals demand it. The similarity of results obtained in the United States, this study, and a related study conducted in Germany suggest that adoption of these practices is an international phenomenon and is likely due to systemic features of the international research and publication processes.},
  number = {3},
  journaltitle = {PLOS ONE},
  date = {2017},
  pages = {e0172792},
  author = {Agnoli, Franca and Wicherts, Jelte M. and Veldkamp, Coosje L. S. and Albiero, Paolo and Cubelli, Roberto},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\WGYN4RV7\\Agnoli et al_2017_Questionable research practices among italian research psychologists.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\NHEG38Y4\\agnoli2017.html},
  bdsk-url-2 = {http://dx.doi.org/10.1371/journal.pone.0172792},
  publisher = {Public Library of Science}
}

@article{Franco2016,
  langid = {english},
  title = {Underreporting in {{Psychology Experiments}}: {{Evidence From}} a {{Study Registry}}},
  volume = {7},
  issn = {1948-5506, 1948-5514},
  url = {http://journals.sagepub.com/doi/10.1177/1948550615598377},
  doi = {10.1177/1948550615598377},
  shorttitle = {Underreporting in {{Psychology Experiments}}},
  number = {1},
  journaltitle = {Social Psychological and Personality Science},
  urldate = {2018-06-19},
  date = {2016},
  pages = {8-12},
  keywords = {publication bias,replication crisis},
  author = {Franco, Annie and Malhotra, Neil and Simonovits, Gabor},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\E4GTCFTJ\\Franco et al_2016_Underreporting in Psychology Experiments.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\JPFIFHJP\\franco2015.html}
}

@article{Franco2014,
  langid = {english},
  title = {Publication Bias in the Social Sciences: {{Unlocking}} the File Drawer},
  volume = {345},
  issn = {0036-8075, 1095-9203},
  url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1255484},
  doi = {10.1126/science.1255484},
  shorttitle = {Publication Bias in the Social Sciences},
  number = {6203},
  journaltitle = {Science},
  urldate = {2018-06-19},
  date = {2014},
  pages = {1502-1505},
  keywords = {publication bias,replication crisis},
  author = {Franco, A. and Malhotra, N. and Simonovits, G.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\WP6WGEWT\\Franco et al_2014_Publication bias in the social sciences.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\RCD8WEBC\\franco2014.html}
}

@article{Mahoney1977,
  title = {Publication {{Prejudices}}: {{An Experimental Study}} of {{Confirmatory Bias}} in the {{Peer Review System}}},
  volume = {1},
  url = {https://dx.doi.org/10.1007/BF01173636},
  doi = {10.1007/BF01173636},
  number = {2},
  journaltitle = {Cognitive Therapy and Research},
  date = {1977},
  pages = {161-175},
  author = {Mahoney, Michael J.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\N5ES6QPC\\Mahoney_1977_Publication Prejudices.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\C76YXINI\\mahoney1977.html}
}

@article{Greenwald1975,
  title = {Consequences of {{Prejudice Against}} the {{Null Hypothesis}}},
  volume = {82},
  number = {1},
  journaltitle = {Psychological Bulletin},
  date = {1975},
  pages = {1-20},
  author = {Greenwald, Anthony G.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\WM73BCC2\\Greenwald_1975_Consequences of Prejudice Against the Null Hypothesis.pdf}
}

@article{Sterling1995,
  title = {Publication {{Decisions Revisited}}: {{The Effect}} of the {{Outcome}} of {{Statistical Tests}} on the {{Decision}} to {{Publish}} and {{Vice Versa}}},
  volume = {49},
  issn = {00031305},
  url = {https://www.jstor.org/stable/2684823?origin=crossref},
  doi = {10.2307/2684823},
  shorttitle = {Publication {{Decisions Revisited}}},
  number = {1},
  journaltitle = {The American Statistician},
  shortjournal = {The American Statistician},
  urldate = {2019-08-15},
  date = {1995-02},
  pages = {108},
  keywords = {publication bias,meta-science,NHST},
  author = {Sterling, T. D. and Rosenbaum, W. L. and Weinkam, J. J.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\D4UQKGT5\\Sterling et al_1995_Publication Decisions Revisited.pdf;C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\2XT7XL2I\\sterling1995.html}
}

@article{Atkinson1982,
  langid = {english},
  title = {Statistical Significance, Reviewer Evaluations, and the Scientific Process: {{Is}} There a (Statistically) Significant Relationship?},
  volume = {29},
  issn = {0022-0167},
  url = {http://content.apa.org/journals/cou/29/2/189},
  doi = {10.1037/0022-0167.29.2.189},
  shorttitle = {Statistical Significance, Reviewer Evaluations, and the Scientific Process},
  number = {2},
  journaltitle = {Journal of Counseling Psychology},
  urldate = {2020-01-05},
  date = {1982},
  pages = {189-194},
  author = {Atkinson, Donald R. and Furlong, Michael J. and Wampold, Bruce E.},
  file = {C\:\\Users\\20176208\\Documents\\Zotero_library\\storage\\UAZIWGXE\\Atkinson et al. - 1982 - Statistical significance, reviewer evaluations, an.pdf}
}


